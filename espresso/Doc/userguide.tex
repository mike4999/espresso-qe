\documentclass[12pt,a4paper]{article}
 \def\version{4.1}

 \usepackage{epsfig}
 \usepackage{html}
  %\def\htmladdnormallink#1#2{#1}

\begin{document} 
\author{}
\date{}
\title{
 \epsfig{figure=quantum_espresso,width=5cm}\hskip 2cm
 \epsfig{figure=democritos,width=6cm}\vskip 2cm
  % title
  \huge User's Guide for Quantum ESPRESSO \smallskip
  \Large (version \version)
}
\maketitle

\tableofcontents

\section{Introduction}

This guide covers the installation and usage of Quantum ESPRESSO
(opEn-Source Package for Research in Electronic Structure, Simulation,
and Optimization), version \version.

The Quantum ESPRESSO distribution contains the following core packages 
for the calculation of electronic-structure properties within
Density-Functional Theory, using a Plane-Wave basis set and pseudopotentials:
\begin{itemize}
  \item PWscf (Plane-Wave Self-Consistent Field),
  \item CP (Car-Parrinello).
\end{itemize}
It also includes the following more specialized packages:
\begin{itemize}
  \item PHonon:
        phonons with Density-Functional Perturbation Theory,
  \item PostProc: various utilities for data prostprocessing,
  \item PWcond:
        ballistic conductance,
  \item GIPAW  (Gauge-Independent Projector Augmented Waves):
        EPR g-tensor and NMR chemical shifts,
  \item XSPECTRA:
        K-edge X-ray adsorption spectra,
  \item vdW:
        (experimental) dynamic polarizability, 
  \item Wannier90:
        maximally localized Wannier functions.
\end{itemize}
Finally, the following auxiliary codes are included:
\begin{itemize}
\item PWgui (Graphical User Interface for PWscf): 
      a graphical interface for producing input data files for PWscf,
\item atomic: 
      a program for atomic calculations and generation of pseudopotentials,
\item iotk:
      an Input-Output ToolKit.
\end{itemize}
This guide documents PWscf, CP, PHonon, PostProc, PWcond. 
The remaining packages have separate documentation.

The Quantum ESPRESSO codes work on many different types of 
Unix machines,
including parallel machines using Message Passing Interface (MPI).
Running Quantum ESPRESSO on Mac OS X and MS-Windows is also possible: 
see section \ref{installation}, ``Installation''.

Further documentation, beyond what is provided in this guide, can be found in:
\begin{itemize}
  \item the Quantum ESPRESSO Wiki\\
   (http://www.quantum-espresso.org/wiki/index.php/Main\_Page) ;
  \item the Doc/ directory of the Quantum ESPRESSO distribution,
   containing a detailed description of all input data for all codes
   in the INPUT\_* files (in .txt, .html, .pdf formats);
\item the pw\_forum mailing list (pw\_forum@pwscf.org).
   You can subscribe to this list and browse and search its archives 
   from the Quantum ESPRESSO web site 
   (http://www.quantum-espresso.org/tools.php).\\
   Only subscribed users can post. Please search the archives 
   before posting: your question may have already been answered.
\end{itemize}

This guide does not explain solid state physics and its computational methods.
If you want to learn that, you should read a good textbook, such as e.g.
the book by Richard Martin:
{\em Electronic Structure: Basic Theory and Practical Methods},
Cambridge University Press (2004).

\subsection{What can Quantum ESPRESSO do}

PWscf can currently perform the following kinds of calculations:
\begin{itemize}
  \item ground-state energy and one-electron (Kohn-Sham) orbitals
  \item atomic forces, stresses, and structural optimization
  \item molecular dynamics on the ground-state Born-Oppenheimer surface, 
   also with variable cell
  \item Nudged Elastic Band (NEB) and Fourier String Method Dynamics (SMD)
  for energy barriers and reaction paths
  \item macroscopic polarization and finite electric fields via 
  the modern theory of polarization (Berry Phases)
\end{itemize}
All of the above works for both insulators and metals, 
in any crystal structure, for many exchange-correlation functionals
(including spin polarization, DFT+U, exact exchange), for
norm-conserving (Hamann-Schluter-Chiang) pseudopotentials in 
separable form or Ultrasoft (Vanderbilt) pseudopotentials 
or Projector Augmented Waves (PAW) method.
Non-collinear magnetism and spin-orbit interactions 
are also implemented.  
Finite electric fields are implemented also using a supercell approach.

PHonon can perform the following types of calculations:
\begin{itemize}
  \item phonon frequencies and eigenvectors at a generic wave vector,
  using Density-Functional Perturbation Theory
  \item effective charges and dielectric tensors
  \item electron-phonon interaction coefficients for metals
  \item interatomic force constants in real space
  \item third-order anharmonic phonon lifetimes
  \item Infrared and Raman (nonresonant) cross section
\end{itemize}
PHonon can be used whenever PWscf can be used, with the exceptions of
DFT+U and exact exchange. PAW is not implemented for higher-order 
response calculations.

The package QHA (Quasi-Harmonic Approximation) for vibrational
free energy calculations
was contributed by Eyvaz Isaev (Moscow Steel and Alloy Inst and
Linkoping and Uppsala Univ.)

PostProc can perform the following types of calculations:
\begin{itemize}
  \item Scanning Tunneling Microscopy (STM) images;
  \item plots of Electron Localization Functions (ELF);
  \item Density of States (DOS) and Projected DOS (PDOS);
  \item L\"owdin charges;
  \item planar and spherical averages;
\end{itemize}
plus interfacing with a number of graphical utilities and with 
external codes.

\subsection{People}

In the following, the cited affiliation is the one where the last known 
contribution was done and may no longer be valid.

The maintenance and further development of the Quantum ESPRESSO code
is promoted by the DEMOCRITOS National Simulation Center 
of CNR-INFM
(Italian Institute for Condensed Matter Physics) under the 
coordination of
Paolo Giannozzi (Univ.Udine, Italy), with the strong support
of the CINECA National Supercomputing Center in Bologna under 
the responsibility of Carlo Cavazzoni.
     
The PWscf package (originally including PHonon and PostProc)
was originally developed by Stefano Baroni, Stefano
de Gironcoli, Andrea Dal Corso (SISSA), Paolo Giannozzi, and many others.
We quote in particular:
\begin{itemize}
  \item Matteo Cococcioni (MIT) for DFT+U implementation;
  \item David Vanderbilt's group at Rutgers for Berry's phase calculations;
  \item Michele Lazzeri (Paris VI) for the 2n+1 code and Raman cross section
 calculation with 2nd-order response;
  \item Ralph Gebauer (ICTP, Trieste) and Adriano Mosca Conte (SISSA, Trieste) 
for noncolinear magnetism;
  \item Andrea Dal Corso for spin-orbit interactions;
  \item Carlo Sbraccia (Princeton) for NEB, Strings method, Metadynamics, for 
improvements to structural optimization and to many other parts of the code.
  \item Paolo Umari (Democritos) for finite electric fields;
  \item Renata Wentzcovitch (Univ.Minnesota) for variable-cell molecular 
dynamics;
  \item Lorenzo Paulatto (SISSA) for PAW implementation, built upon previous 
work by Guido Fratesi (Univ.Milano Bicocca) and Riccardo Mazzarello (SISSA);
\item Filippo Spiga (Univ. Milano Bicocca) for mixed SMP-OpenMP
 parallelization;
 \item Ismaila Dabo (INRIA, Palaiseau) for electrostatics with
 free boundary conditions;
 \item Andrea Dal Corso for Ultrasoft, noncollinear, spin-orbit
 extensions to PHonon.
\end{itemize}

The CP code is based on the original code written by Roberto Car and
Michele Parrinello. CP was developed by Alfredo Pasquarello (IRRMA, Lausanne),
Kari Laasonen (Oulu), Andrea Trave, Roberto Car (Princeton), 
Nicola Marzari (MIT), Paolo Giannozzi, and others.
FPMD, later merged with CP, was developed by Carlo Cavazzoni, 
Gerardo Ballabio (CINECA), Sandro Scandolo (ICTP), 
Guido Chiarotti (SISSA), Paolo Focher, and others.
We quote in particular:
\begin{itemize}
  \item Carlo Sbraccia (Princeton) for NEB and Metadynamics;
  \item Manu Sharma (Princeton) and Yudong Wu (Princeton) for maximally 
localized Wannier functions and dynamics with Wannier functions;
  \item Paolo Umari (MIT) for finite electric fields and conjugate gradients;
  \item Paolo Umari and Ismaila Dabo for ensemble-DFT;
  \item Xiaofei Wang (Princeton) for META-GGA;
  \item The Autopilot feature was implemented by Targacept, Inc.
\end{itemize}
Other packages in Quantum ESPRESSO:
\begin{itemize}
\item
PWcond was written by Alexander Smogunov (SISSA) and Andrea Dal Corso.
\item
GIPAW (http://www.gipaw.org) was written by Davide Ceresoli
(MIT), Ari Seitsonen, Uwe Gerstmann,  Francesco Mauri (Univ.
Paris VI) .
\item
PWgui was written by Anton Kokalj (IJS Ljubljana) and is based on his
GUIB concept (http://www-k3.ijs.si/kokalj/guib/).
\item
atomic was written by Andrea Dal Corso and it is the result 
of many additions to the original code by Paolo Giannozzi 
and others. Lorenzo Paulatto wrote the extension to PAW.
\item
iotk (http://www.s3.infm.it/iotk) was written by Giovanni Bussi 
(ETHZ and S3 Modena).
\item
Wannier90 (http://www.wannier.org/) was written by A. Mostofi, J. Yates, 
Y.-S Lee (MIT).
\item
XSPECTRA was written by Matteo Calandra (Univ. Paris VI).
\end{itemize}
Other relevant contributions to Quantum ESPRESSO:
\begin{itemize}
  \item Gerardo Ballabio wrote the first "configure" for Quantum ESPRESSO.
  \item The calculation of the finite (imaginary) frequency molecular
polarizability using the approximated Thomas-Fermi  + von Weizaecker
scheme (VdW) was contributed by Huy-Viet Nguyen (SISSA).
  \item The calculation of RPA frequency-dependent complex dielectric
function was contributed by Andrea Benassi (S3 Modena).
  \item The initial BlueGene porting was done by Costas Bekas and
Alessandro Curioni (IBM Zurich).
  \item Audrius Alkauskas (IRRMA), 
Simon Binnie (Univ. College London), Davide Ceresoli,
Andrea Ferretti (S3), Guido Fratesi, Axel Kohlmeyer (UPenn),
Konstantin Kudin (Princeton), Sergey Lisenkov (Univ.Arkansas), 
Nicolas Mounet (MIT), Guido Roma (CEA), Sylvie Stucki (IRRMA),
Pascal Thibaudeau (CEA), 
answered questions on the mailing list, found bugs, helped in 
porting to new architectures, wrote some code.
\end{itemize}

An alphabetical list of further contributors includes: Dario Alf\`e, 
Alain Allouche, 
Francesco Antoniella, Mauro Boero, Nicola Bonini, Claudia Bungaro, 
Paolo Cazzato, Gabriele Cipriani, Jiayu Dai, Cesar Da Silva, 
Alberto Debernardi, Gernot Deinzer, 
Martin Hilgeman,  Yosuke Kanai, Nicolas Lacorne, Stephane Lefranc,
Kurt Maeder, Andrea Marini, 
Pasquale Pavone,  Mickael Profeta, Kurt Stokbro, 
Paul Tangney, 
Antonio Tilocca, Jaro Tobik, 
Malgorzata Wierzbowska, Silviu Zilberman, 
and let us apologize to everybody we have forgotten.
    
This guide was mostly written by Paolo Giannozzi.
Gerardo Ballabio and Carlo Cavazzoni wrote the section on CP.

\subsection{Contacts}

The web site for Quantum ESPRESSO is http://www.quantum-espresso.org/.
Releases and patches of Quantum ESPRESSO can be downloaded from this
site or following the links contained in it. The main entry point for 
developers is the QE-forge web site: http://www.qe-forge.org/.

Announcements about new versions of Quantum ESPRESSO are available 
via a low-traffic mailing list Pw\_users: (pw\_users@pwscf.org). You can
subscribe (but not post) to this list from the Quantum ESPRESSO web site.
    
The recommended place where to ask questions about installation and
usage of Quantum ESPRESSO, and to report bugs, is the Pw\_forum mailing
list (pw\_forum@pwscf.org). Here you can obtain help from the developers
and many knowledgeable users. You can browse and search its archive from 
the Quantum ESPRESSO web site, but you have to subscribe in order to post 
to the list.
Please search the archives before posting: your question may have already
been answered.
{\bf Important notice:} only messages that appear to come from the 
registered user's e-mail address, in its {\em exact form}, will be
accepted. Messages "waiting for moderator approval" are automatically 
deleted with no further processing (sorry, too much spam). In case of 
trouble, carefully check that your return e-mail is the correct one 
(i.e. the one you used to subscribe).

The Pw\_forum mailing list is also the recommanded place where to 
contact the developers of Quantum ESPRESSO.
 
\subsection{Terms of use}

Quantum ESPRESSO is free software, released under the 
GNU General Public License 
(http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt, 
or the file License in the distribution).
    
All trademarks mentioned in this guide belong to their respective owners.
    
We shall greatly appreciate if scientific work done using this code will 
contain an explicit acknowledgment and the following reference:
\begin{quote}
P. Giannozzi et al., {\bf TO BE UPDATED}
\end{quote}
Note the form {\sc Quantum ESPRESSO} for textual citations of the code.
Pseudopotentials should be cited as (for instance)

[ ] We used the pseudopotentials C.pbe.rrjkus.UPF
and O.pbe.vbc.UPF from the http://www.quantum-espresso.org 
distribution.

\section{Installing Quantum ESPRESSO}

\subsection{Download}
 
Presently, the Quantum ESPRESSO package is only distributed in source form; 
some precompiled executables (binary files) are provided only for PWgui. 
Stable releases of the Quantum ESPRESSO source package (current version 
is \version) can be downloaded from this URL: \\
http://www.quantum-espresso.org/download.php .

Uncompress and unpack the distribution using the command:
\begin{verbatim}
     tar zxvf espresso-4.1.tar.gz
\end{verbatim}
(a hyphen before "zxvf" is optional). If your version of "tar" 
doesn't recognize the "z" flag:
\begin{verbatim}
     gunzip -c espresso-4.1.tar.gz | tar xvf -
\end{verbatim}
A directory espresso-\version/, containing the distribution, will be created. 
Occasionally, patches for the current version, fixing some errors and bugs,
may be distributed as a "diff" file. In order to install a patch (for instance):\begin{verbatim}
   cd espresso-4.1/
   patch -p1 < /path/to/the/diff/file/patch-file.diff
\end{verbatim}

If more than one patch is present, they should be applied in the correct order.

Daily snapshots of the development version can be downloaded from the
developers' site qe-forge.org: follow the link ''Quantum ESPRESSO'',
then ''SCM''. Beware: the development version is, well, under
development: use at your own risk! The bravest may access the 
development version via anonymous CVS 
(Concurrent Version System): see the developer manual, section
''Using CVS''.

Directory structure of Quantum ESPRESSO. Common part for all packages:
\begin{verbatim}
  Modules/    source files for modules that are common to all programs
  include/    files *.h included by fortran source files
  clib/       external libraries written in C
  flib/       external libraries written in Fortran
  iotk/       Input/Output Toolkit
  install/    machine-dependent makefiles
  pseudo/     pseudopotential files used by examples
  upftools/   source files for converters to unified pseudopotential format (UPF)
  examples/   sample input and output files
  tests/      automated tests
  Doc/        documentation
\end{verbatim}
Specific to each package:
\begin{verbatim}
  PW/         PWscf: source files for scf calculations (pw.x)
  pwtools/    PWscf: source files for miscellaneous analysis programs
  PP/         PostProc: source files for post-processing of pw.x data file
  PH/         PHonon: source files for phonon calculations (ph.x) and analysis programs
  Gamma/      PHonon: source files for Gamma-only phonon calculation (phcg.x)
  D3/         PHonon: source files for third-order derivative calculations (d3.x)
  PWCOND/     PWcond: source files for conductance calculations (pwcond.x)
  vdW/        VdW: source files for calculation of the molecular polarizability
              at finite (imaginary) frequency using approximated Thomas-Fermi
              + von Weizacker scheme
  CPV/        CP: source files for Car-Parrinello code (cp.x)
  atomic/     Source files for the pseudopotential generation package (ld1.x)
  atomic_doc/ Documentation, tests and examples for atomic
  GUI/        PWGui: Graphical User Interface
\end{verbatim}

\subsection{Installation}

To install Quantum ESPRESSO from source, you need C and Fortran-95
compilers (Fortran-90 is not sufficient, but most ``Fortran-90''
compilers are actually Fortran-95-compliant). If you don't have a
commercial Fortran-95 
compilers, you may install the free g95 compiler: (http://www.g95.org/) or
the GNU fortran compiler gfortran: (http://www.gfortran.org/).
Note that both the C and the F90 compilers must be in your PATH, or else
their full path must be explicitly given.

You also need a minimal Unix environment: basically, a command shell (e.g.,
bash or tcsh) and the utilities make, awk and sed. MS-Windows users need
to have Cygwin (a UNIX environment which runs under Windows) installed:
see http://www.cygwin.com/. Note that the scripts contained in the distribution
assume that the local  language is set to the standard, i.e. "C"; other
 settings 
may break them. Use
\begin{verbatim}
    export LC_ALL=C
\end{verbatim}
or
\begin{verbatim}
    setenv LC_ALL C
\end{verbatim}
to prevent any problem when running scripts (including installation scripts).

Instructions for the impatient:
\begin{verbatim}
    cd espresso-4.1/
    ./configure
     make all
\end{verbatim}
Executable programs (actually, symlinks to them) will be placed in the bin/
subdirectory.
    
If you have problems or would like to tweak the default settings, read the
detailed instructions below.

\subsection{Configure}

To install the Quantum ESPRESSO source package, run the configure
script. It will (try to) detect compilers and libraries available on
your machine, and set up things accordingly. Presently it is expected
to work on most Linux 32- and 64-bit PCs (all Intel and AMD CPUs), PC
clusters, IBM SP machines, SGI Origin and Altix, some HP-Compaq Alpha
machines, NEC SX, Cray X1, Mac OS X, MS-Windows PCs. It may work with
some assistance also on other architectures (see below). 
    
For cross-compilation, you have to specify the target machine with the
--host option (see below). This feature has not been extensively
tested, but we had at least one successful report (compilation for NEC
SX6 on a PC). 
    
Specifically, configure generates the following files:
\begin{verbatim}
    make.sys:           compilation rules and flags
    configure.msg:      a report of the configuration run
    include/fft_defs.h: defines fortran variable for C pointer
    include/c_defs.h:   defines C to fortran calling convention
                        and a few more things used by C files
\end{verbatim}
configure.msg is only used by configure to print its final report. It isn't
needed for compilation. NOTA BENE: unlike previous versions, configure
no longer runs the makedeps.sh shell script that updates dependencies. If
you modify the program sources, run makedeps.sh or type make depend to
update files make.depend in the various subdirectories.
    
You should always be able to compile the Quantum ESPRESSO suite
of programs without having to edit any of the generated files. However you
may have to tune configure by specifying appropriate environment variables
and/or command-line options. Usually the most tricky part is to get external
libraries recognized and used: see the ''Libraries''
section for details and hints.

Environment variables may be set in any of these ways:
\begin{verbatim}
     export VARIABLE=value               # sh, bash, ksh
     ./configure
     setenv VARIABLE value               # csh, tcsh
     ./configure
     ./configure VARIABLE=value          # any shell
\end{verbatim}
Some environment variables that are relevant to configure are:
\begin{verbatim}
     ARCH:         label identifying the machine type (see below)
     F90, F77, CC: names of Fortran 95, Fortran 77, and C compilers
     MPIF90:       name of parallel Fortran 95 compiler (using MPI)
     CPP:          source file preprocessor (defaults to $CC -E)
     LD:           linker (defaults to $MPIF90)
     CFLAGS, FFLAGS, F90FLAGS, CPPFLAGS, LDFLAGS: compilation flags
     LIBDIRS:      extra directories to search for libraries (see below)
\end{verbatim}
For example, the following command line:
\begin{verbatim}
     ./configure MPIF90=mpf90 FFLAGS="-O2 -assume byterecl" \
                  CC=gcc CFLAGS=-O3 LDFLAGS=-static
\end{verbatim}
instructs configure to use mpf90 as Fortran 95 compiler with flags -O2
-assume byterecl, gcc as C compiler with flags -O3, and to link with flags
-static. Note that the value of FFLAGS must be quoted, because it contains
spaces. NOTA BENE: do not pass compiler names with the leading path
included. "F90=f90xyz" is ok, "F90=/path/to/f90xyz" is not.

If your machine type is unknown to configure, you may use the ARCH
variable to suggest an architecture among supported ones. Try the one that
looks more similar to your machine type; you'll probably have to do some
additional tweaking. Currently supported architectures are:
\begin{verbatim}
      ia32:   Intel 32-bit machines (x86) running Linux
      ia64:   Intel 64-bit (Itanium) running Linux
      x86_64: Intel and AMD 64-bit running Linux - see note below
      aix:    IBM AIX machines
      mips:   SGI MIPS machines
      alpha:  HP-Compaq alpha machines
      alinux: HP-Compaq alpha running Linux
      sparc:  Sun SPARC machines
      solaris:PC's running SUN-Solaris
      crayx1: Cray X1 machines
      macppc: Apple PowerPC machines running Mac OS X
      mac686: Apple Intel machines running Mac OS X
      cygwin: MS-Windows PCs with Cygwin
      necsx:  NEC SX-6 and SX-8 machines
      ppc64:  Linux PowerPC machines, 64 bits
      ppc64-mn:as above, with IBM xlf compiler
\end{verbatim}
{\em Note}: x86\_64 replaces amd64 since v.4.1. 
Finally, configure recognizes the following command-line options:
\begin{verbatim}
--disable-parallel: compile serial code, even if parallel compiler available
--host=target :     specify target machine for cross-compilation.
                    target is a string identifying the architecture you want
                    to compile for; run config.guess on the target machine
--disable-shared:   don't use shared libraries: generate static executables
--enable-shared:    use shared libraries
\end{verbatim}
The latter two options actually work only in a few specific cases.
If you want to modify the configure script (advanced users only!), see the 
Developer Manual, ''How to edit the configure script'' section. 
You will need GNU Autoconf (http://www.gnu.org/software/autoconf/)
installed.

\subsubsection{Libraries}

Quantum ESPRESSO makes use of the following external libraries:
\begin{itemize}
\item BLAS (http://www.netlib.org/blas/) and 
\item LAPACK (http://www.netlib.org/lapack/) for linear algebra 
\item FFTW (http://www.fftw.org/) for Fast Fourier Transforms
\end{itemize}
A copy of the needed routines is provided with the distribution. However,
when available, optimized vendor-specific libraries can be used instead: this
often yields huge performance gains.

\paragraph{BLAS and LAPACK} 
Quantum ESPRESSO can use the following architecture-
specific replacements for BLAS and LAPACK:
\begin{verbatim}
      MKL for Intel Linux PCs
      ACML for AMD Linux PCs
      ESSL for IBM machines
      complib.sgimath for SGI Origin
      SCSL for SGI Altix
      SUNperf for Sun
      cxml for HP-Compaq Alphas.
\end{verbatim}
If none of these is available, we suggest that you use the optimized ATLAS
library (http://math-atlas.sourceforge.net/). Note that ATLAS is not
a complete replacement for LAPACK: it contains all of the BLAS, plus the
LU code, plus the full storage Cholesky code. Follow the instructions in the
ATLAS distributions to produce a full LAPACK replacement.
    
Sergei Lisenkov reported success and good performances with optimized
BLAS by Kazushige Goto. They can be freely downloaded  (but not redistributed):
http://www.cs.utexas.edu/users/flame/goto/

\paragraph{FFT}
Quantum ESPRESSO can use the following vendor-specific FFT libraries:
\begin{verbatim}
      IBM ESSL
      SGI SCSL
      SUN sunperf
      NEC ASL
      AMD ACML
\end{verbatim}
If none of the above is available, you should use FFTW, choosing
before compilation whether to load the built-in copy of FFTW 
or an external v.3 FFTW library.
Configure will first search for vendor-specific FFT libraries;
if none is found, it will search for an external FFTW v.3 library;
if none is found, it will fall back to the internal  copy of FFTW.
Appropriate precompiling options will be set in all cases:
\begin{verbatim}
      __FFTW      internal FFTW
      __FFTW3     external FFTW v.3
      __SCSL      SGI SCSL
      __SUNPERF   SUN sunperf 
      __ESSL      IBM ESSL
        ASL       NEC ASL
\end{verbatim}
If you have recent versions of MKL installed, you may try the 
FFTW interface provided with MKL. You will have to compile them
(they come in source form with the MKL library)
and to modify the make.sys accordingly (MKL must linked {\em after}
the FFTW-MKL interface)

If everything else fails, you'll have to modify the make.sys file
manually: see the section on Manual configuration. 

\paragraph{MPI libraries} 
For parallel execution, Quantum ESPRESSO uses the MPI libraries.
In well-configured machine, configure should find the appropriate
parallel compiler for you, and this should find the appropriate
libraries. Since often this doesn't 
happen, especially on PC clusters, see the section on 
''Linux PC clusters with MPI''.

\paragraph{Other libraries}
Quantum ESPRESSO can use the MASS vector math
library from IBM, if available (only on AIX).
    
The configure script attempts to find optimized libraries, but may fail
if they have been installed in non-standard places. You should examine
the final value of BLAS\_LIBS, LAPACK\_LIBS, FFT\_LIBS, MPI\_LIBS (if needed),
MASS\_LIBS (IBM only), either in the output of configure or in the generated
make.sys, to check whether it found all the libraries that you intend to use.
    
If some library was not found, you can specify a list of directories to search
in the environment variable LIBDIRS, and rerun configure; directories in the
list must be separated by spaces. For example:
\begin{verbatim}
   ./configure LIBDIRS="/opt/intel/mkl70/lib/32 /usr/lib/math"
\end{verbatim}
If this still fails, you may set some or all of the *\_LIBS variables manually
and retry. For example:
\begin{verbatim}
   ./configure BLAS_LIBS="-L/usr/lib/math -lf77blas -latlas_sse"
\end{verbatim}
Beware that in this case, configure will blindly accept the specified value,
and won't do any extra search. 
    
{\em Please note}: If you change any settings after a previous (successful or
failed) compilation, you must run "make clean" before recompiling, unless you
know exactly which routines are affected by the changed settings and how to
force their recompilation.

\subsubsection{Manual configuration}

If "configure" stops before the end, and you don't find a way to fix
it, you have to write working "make.sys", "include/fft\_defs.h" and
"include/c\_defs.h" files. 
For the latter two files, follow the explanations in 
"include/defs.h.README". 

If "configure" has run till the end, you should need only to
edit "make.sys". A few templates (each for a different machine type)
are provided in the install/ directory: they have names of the
form Make.system, where "system" is a string identifying the 
architecture and compiler.

If you have the Intel compiler ifc v.6 or earlier, you will have to 
run the script ifcmods.sh.
    
Most likely (and even more so if there isn't an exact match to your 
machine type), you'll have to tweak make.sys by hand. In particular, 
you must
specify the full list of libraries that you intend to link to.
    
{\em NOTA BENE}:
If you modify the program sources, run the script makedeps.sh
or type make depend to update files make.depend in the various 
subdirectories.

\subsection{Compilation}

There are a few adjustable parameters in Modules/parameters.f90. The
present values will work for most cases. All other variables are dynamically
allocated: you do not need to recompile your code for a different system.
    
At your option, you may compile the complete Quantum ESPRESSO
suite of programs (with make all), or only some specific programs.
    
make with no arguments yields a list of valid compilation targets. Here is
a list:
\begin{itemize}
\item make pw produces PW/pw.x\\
pw.x calculates electronic structure, structural optimization, molecular dynamics, barriers with NEB.
\item make ph produces PH/ph.x\\
ph.x calculates phonon frequencies and displacement patterns,
dielectric tensors, effective charges (uses data produced by pw.x). 
\item make d3 produces D3/d3.x\\
d3.x calculates anharmonic phonon lifetimes (third-order derivatives
of the energy), using data produced by pw.x and ph.x (Ultrasoft
pseudopotentials not supported). 
\item make gamma produces Gamma/phcg.x\\
phcg.x is a version of ph.x that calculates phonons at q = 0 using
conjugate-gradient minimization of the density functional expanded to
second-order. Only the $\Gamma$ (q = 0) point is used for Brillouin zone
integration. It is faster and takes less memory than ph.x, but does
not support Ultrasoft pseudopotentials. 
\item  make pp produces several codes for data postprocessing, in PP/
  (see list below). 
\item make tools produces several utility programs in pwtools/ (see
  list below).  
\item make pwcond produces PWCOND/pwcond.x\\
 for ballistic conductance calculations.
\item make pwall produces all of the above.
\item make ld1 produces code atomic/ld1.x\\
for pseudopotential generation (see specific documentation in atomic\_doc/).
\item make upf produces utilities for pseudopotential conversion in
  directory upftools/ (see section 4, ``Pseudopotentials''). 
\item make cp produces the Car-Parrinello code CP in CPV/cp.x. and the
  postprocessing code CPV/cppp.x. 
\item make all produces all of the above.
\end{itemize}
For the setup of the GUI, refer to the PWgui-X.Y.Z /INSTALL file, where
X.Y.Z stands for the version number of the GUI (should be the same as the
general version number). If you are using the CVS sources, see
the GUI/README file instead.
   
The codes for data postprocessing in PP/ are:
\begin{itemize}
\item pp.x extracts the specified data from files produced by pw.x,
  prepares data for plotting by writing them into formats that can be
  read by several plotting programs. 
\item bands.x extracts and reorders eigenvalues from files produced by
  pw.x for band structure plotting 
\item projwfc.x calculates projections of wavefunction over atomic
  orbitals, performs L\"wdin population analysis and calculates
  projected density of states. These can be summed using auxiliary
  code sumpdos.x. 
\item dipole.x calculates the dipole moment for isolated systems
  (molecules) and the Makov-Payne correction for molecules in
  supercells (beware: meaningful results only if the charge density is
  completely contained into the Wigner-Seitz cell) 
\item plotrho.x produces PostScript 2-d contour plots
\item plotband.x reads the output of bands.x, produces band structure
  PostScript plots
\item average.x calculates planar averages of quantities produced by
  pp.x (potentials, charge, magnetization densities,...) 
\item voronoy.x divides the charge density into Voronoy polyhedra
  (obsolete, use at your own risk) 
\item dos.x calculates electronic Density of States (DOS)
\item pw2wan.x: interface with code WanT for calculation of transport
  properties via Wannier functions: see \\
  http://www.wannier-transport.org/ 
\item pmw.x generates Poor Man's Wannier functions, to be used in
  DFT+U calculations 
\item pw2casino.x: interface with CASINO code for Quantum Monte Carlo
  calculation (http://www.tcm.phy.cam.ac.uk/\~{}mdt26/casino.html). 
\end{itemize}
The utility programs in pwtools/ are:
\begin{itemize}
\item dynmat.x applies various kinds of Acoustic Sum Rule (ASR),
  calculates LO-TO splitting at q = 0 in insulators, IR and Raman
  cross sections (if the coefficients have been properly calculated),
  from the dynamical matrix produced by ph.x 
\item q2r.x calculates Interatomic Force Constants (IFC) in real space
  from dynamical matrices produced by ph.x on a regular q-grid 
\item  matdyn.x produces phonon frequencies at a generic wave vector
  using the IFC file calculated by q2r.x; may also calculate phonon DOS 
\item fqha.x for quasi-harmonic calculations 
\item lambda.x calculates the electron-phonon coefficient $\lambda$ and the
  function $\alpha^2F(\omega)$ 
\item dist.x calculates distances and angles between atoms in a cell,
  taking into account periodicity 
\item ev.x fits energy-vs-volume data to an equation of state
\item kpoints.x produces lists of k-points
\item pwi2xsf.sh, pwo2xsf.sh process respectively input and output
  files (not data files!) for pw.x and produce an XSF-formatted file
  suitable for plotting with XCrySDen, a powerful crystalline and
  molecular structure visualization program
  (http://www.xcrysden.org/). BEWARE: the pwi2xsf.sh shell script
  requires the pwi2xsf.x executables to be located somewhere in your
  \$PATH. 
\item band\_plot.x: undocumented and possibly obsolete 
\item bs.awk, mv.awk are scripts that process the output of pw.x (not
data files!). Usage: 
\begin{verbatim}
         awk -f bs.awk < my-pw-file > myfile.bs
         awk -f mv.awk < my-pw-file > myfile.mv
\end{verbatim}
The files so produced are suitable for use with xbs, a very simple
X-windows utility to display molecules, available at:\\
http://www.ccl.net/cca/software/X-WINDOW/xbsa/README.shtml 
\item path\_int.sh/path\_int.x: utility to generate, starting from a
  path (a set of images), a new one with a different number of
  images. The initial and final points of the new path can differ from
  those in the original one. Useful for NEB calculations. 
\item kvecs\_FS.x, bands\_FS.x: utilities for Fermi Surface plotting
  using XCrySDen
\end{itemize}

\paragraph{Other utilities}
VdW/ contains the sources for the calculation of the finite (imaginary)
frequency molecular polarizability using the approximated Thomas-Fermi
+ von Weiz\"acker scheme, contributed by H.-V. Nguyen (Sissa and
Hanoi University). Compile with make vdw, executables in VdW/vdw.x, no
documentation yet, but an example in examples/example34.  

\subsection{Running examples}

As a final check that compilation was successful, you may want to run some or
all of the examples contained within the examples directory of the 
Quantum ESPRESSO distribution. Those examples try to exercise all the programs
and features of the Quantum ESPRESSO package. A list of examples and
of what each example does is contained in examples/README. For details,
see the README file in each example's directory. If you find that any relevant
feature isn't being tested, please contact us (or even better, write and send
us a new example yourself !).
     
To run the examples, you should follow this procedure:

    
1. Go to the examples directory and edit the environment variables
   file, setting the following variables as needed: 
\begin{verbatim}
   BIN_DIR= directory where Quantum ESPRESSO executables reside
   PSEUDO_DIR= directory where pseudopotential files reside
   TMP_DIR= directory to be used as temporary storage area
\end{verbatim}
If you have downloaded the full Quantum ESPRESSO distribution, you may set 
BIN\_DIR=\$TOPDIR/bin and PSEUDO\_DIR=\$TOPDIR/pseudo, where =\$TOPDIR
is the root of the Quantum ESPRESSO source tree. In order to be able
to run all the examples, the PSEUDO\_DIR directory must contain all the
needed pseudopotentials. 
If any of these are missing, you can download them (and many others)
from the Pseudopotentials Page of the Quantum ESPRESSO web site
(http://www.quantum-espresso.org/pseudo.php). TMP\_DIR must be a
directory you 
have read and write access to, with enough available space to host the
temporary files produced by the example runs, and possibly offering
high I/O performance (i.e., don't use an NFS-mounted directory). 

2. If you have compiled the parallel version of Quantum ESPRESSO (this
is the default if parallel libraries are detected), you will usually
have to specify a driver program (such as poe or mpiexec) and the
number of processors: see section ''Running on parallel machines' for
details. In order to do that, edit again the environment variables file
and set the PARA\_PREFIX and PARA\_POSTFIX variables as needed. Parallel
executables will be run by a command like this: 
\begin{verbatim}
      $PARA_PREFIX pw.x $PARA_POSTFIX < file.in > file.out
\end{verbatim}
For example, if the command line is like this (as for an IBM SP):
\begin{verbatim}
      poe pw.x -procs 4 < file.in > file.out
\end{verbatim}
you should set PARA\_PREFIX="poe", PARA\_POSTFIX="-procs
4". Furthermore, if your machine does not support interactive use, you
must run the commands specified below through the batch queueing
system installed on that machine. Ask your system administrator for
instructions. 

3. To run a single example, go to the corresponding directory (for
   instance, example/example01) and execute: 
\begin{verbatim}
      ./run_example
\end{verbatim}
This will create a subdirectory results, containing the input and
output files generated by the calculation. Some examples take only a
few seconds to run, while others may require several minutes depending
on your system. To run all the examples in one go, execute:
\begin{verbatim}
      ./run_all_examples
\end{verbatim}
from the examples directory. On a single-processor machine, this
typically takes a few hours. The make\_clean script cleans the
examples tree, by removing all the results subdirectories. However, if
additional subdirectories have been created, they aren't deleted. 

4. In each example's directory, the reference subdirectory contains
verified output files, that you can check your results against. They
were generated on a Linux PC using the Intel compiler. On different
architectures the precise numbers could be slightly different, in
particular if different FFT dimensions are automatically selected. For
this reason, a plain diff of your results against the reference data
doesn't work, or at least, it requires human inspection of the
results. Instead, you can run the  
check\_example script in the examples directory:
\begin{verbatim}
      ./check_example example_dir
\end{verbatim}
where example dir is the directory of the example that you want to
check (e.g., ./check\_example example01). You can specify multiple
directories.  

{\em Note}: check\_example does only a fair job, and only for a few examples. 
For pw.x only, a much better series of automated tests is available in
directory tests/. Edit variables PARA\_PREFIX, PARA\_POSTFIX (if
needed) in file "check\_pw.x.j"; explanations on how to run it 
are in the header of the file.

\subsection{Installation tricks and problems}

\subsubsection{All architectures}

Working fortran-95 and C compilers are needed in order
to compile Quantum ESPRESSO. Most so-called ``fortran-90'' compilers
implement the fortran-95 standard, but older versions may not be
fortran-95 compliant.
    
If you get ``Compiler Internal Error'' or similar messages: your
compiler version is buggy. Try to lower the optimization level, or to
remove optimization just for the routine that has problems. If it
doesn't work, or if you experience weird problems, try to 
install patches for your version of the compiler (most vendors release
at least a few patches for free), or to upgrade to a more recent version.
    
If you get error messages at the loading phase that looks like 
``file XYZ.o: unknown (unrecognized, invalid, wrong, missing, ... ) file
type'', or  ``file format not recognized for file XYZ.a'', one of
the following things have happened:
\begin{enumerate}
\item you have leftover object files from a compilation with another
  compiler: run make clean and recompile. 
\item make does not stop at the first compilation error (it may happen
  in some software configurations). Remove file XYZ.o and look for the
  compilation  error. 
\end{enumerate}
If many symbols are missing in the loading phase: you did not specify the
location of all needed libraries (LAPACK, BLAS, FFTW, machine-specific
optimized libraries). If you did, but symbols are still missing, see below (for
Linux PC). Remember: Quantum ESPRESSO if self-contained (with the exception of
MPI libraries for parallel compilation). If system libraries are missing, the
problem cannot be in Quantum ESPRESSO.

\subsubsection{IBM AIX}
On IBM machines with ESSL libraries installed, there is a 
potential conflict between a few LAPACK routines that are also part of ESSL, 
but with a different calling sequence. The appearence of run-time errors like
\begin{verbatim}
    ON ENTRY TO ZHPEV  PARAMETER NUMBER  1 HAD AN ILLEGAL VALUE
\end{verbatim}
is a signal that you are calling the bad routine. If you have defined 
-D\_\_ESSL you should load ESSL before LAPACK: see variable
LAPACK\_LIBS in make.sys.

\subsubsection{Linux PC}
The web site of Axel Kohlmeyer contains a very informative
section on compiling and running CPMD on Linux. Most of its contents
applies to the Quantum ESPRESSO code as well:\\
http://www.theochem.rub.de/\~{}axel.kohlmeyer/cpmd-linux.html. In
particular, there is a set of ATLAS libraries, containing all of
LAPACK and no external reference to fortran libraries:\\
http://www.theochem.rub.de/\~{}axel.kohlmeyer/cpmd-linux.html\#atlas 
    
It is convenient to create semi-statically linked executables (with only
libc/libm/libpthread linked dynamically). If you want to produce a binary
that runs on different machines, compile it on the oldest machine you have
(i.e. the one with the oldest version of the operating system).

If you get errors like 
\begin{verbatim}
    IPO Error: unresolved : __svml_cos2
\end{verbatim}
at the linking stage, your compiler is optimized to use the SSE
version of sine, cosine etc. contained in the SVML library. Append
'-lsvml' to the list of libraries in your make.sys file (info by Axel
Kohlmeyer, oct.2007). 

\paragraph{Linux PCs with Portland Group compiler (pgf90)}
Quantum ESPRESSO does not work reliably, or not at all, with many
versions of the Portland Group compiler (in particular, v.5.2 and
6.0). Version 5.1 used to work, v.6.1 is reported to work (info from
Paolo Cazzato). Use the latest version of each release of the
compiler, with patches if available: 
see the Portland Group web site,
http://www.pgroup.com/faq/install.htm\#release info

\paragraph{Linux PCs with Pathscale compiler}

Version 2.99 of the Pathscale compiler works and is recognized by
configure, but the preprocessing step:
\begin{verbatim}
   pathcc -E
\end{verbatim}
causes a mysterious error in compilation of iotk and should be replaced by
\begin{verbatim}
   /lib/cpp -P --traditional
\end{verbatim}
The MVAPICH parallel environment with Pathscale compilers also works.
(info by Paolo Giannozzi, July 2008)

\paragraph{Linux PCs with gfortran}

Recent versions of gfortran (e.g. v.4.1 and later) are supported, but
only the basic functionalities have been tested. More advanced ones
may or may not work. In particular: reading files produced by previous
versions of Quantum ESPRESSO may not work, apparently due to a gfortran bug.

\paragraph{Linux PCs with Intel compiler (ifort, formerly ifc)}

If configure doesn't find the compiler, or if you get ``Error loading shared
libraries...'' at run time, you may have forgotten to execute the script that
sets up the correct path and library path. Unless your system manager has
done this for you, you should execute the appropriate script -- located in
the directory containing the compiler executable -- in your
initialization files. Consult the documentation provided by Intel. 
    
Starting from the latests v 8.1 patchlevels, the recommended way to build
semi-statically linked binaries is to use the -i-static flag; for
multi-threaded 
libraries the linker flag would be -i-static -openmp (linking libguide is
no longer needed and the compiler will pick the correct one). The warning:
''feupdateenv is not implemented and will always fail'', showing up in recent 
versions, can be safely ignored. For previous versions, try -static-libcxa 
(this will give an incomplete semi-static link on newer versions).

Each major release of the Intel compiler differs a lot from the previous one.
Do not mix compiled objects from different releases: they may be incompatible.
    
In case of trouble, update your version with the most recent patches,
available via Intel Premier support (registration free of charge for Linux):
http://developer.intel.com/software/products/support/\#premier.

{\bf ifort v.10:} on 64-bit AMD CPUs, at least some versions of ifort 10.1 
miscompile subroutine write\_rho\_xml in Module/xml\_io\_base.f90 with -O2
options. Using -O1 instead solves the problem (info by Carlo
Cavazzoni, March 2008). 

"The intel compiler version 10.1.008 miscompiles a lot of codes (I have proof 
for CP2K and CPMD) and needs to be updated in any case" (info by Axel
Kohlmeter, May 2008).
 
{\bf ifort v.9:} The latest (July 2006) 32-bit version of ifort 9.1
works flawlessy. Earlier versions yielded ``Compiler Internal Error''.

At least some versions of ifort 9.0 have a buggy preprocessor that either
prevents compilation of iotk, or produces runtime errors in cft3. Update
to a more patched version, or modify make.sys to explicitly perform 
preprocessing using /lib/cpp, as in the following example (courtesy from Sergei
Lisenkov):
\begin{verbatim}
   .f90.o:
           $(CPP) $(CPPFLAGS) $< -o $*.F90
           $(MPIF90) $(F90FLAGS) -c $*.F90 -o $*.o

   CPP       = /lib/cpp
   CPPFLAGS  = -P -C -traditional $(DFLAGS) $(IFLAGS)
\end{verbatim}
    
On some versions of RedHat Linux, you may get an obscure error: IPO
link: can not find ``(`` ... , due to a bad system configuration. Add
option -no-ipo to LDFLAGS in file make.sys.

{\bf ifort v.8:} Some releases of ifort 8 yield "Compiler Internal
Error". Update to a more patched version: 8.0.046 for v. 8.0, 8.1.018
for v. 8.1. 

There is a well known problem with ifort 8 and pthreads (that are used
both in Debian Woody and Sarge) that causes "segmentation fault" errors
(info from Lucas Fernandez Seivane). Version 7 did not have this problem.

{\bf ifc v.7:} Some releases of ifc 7.0 and 7.1 yield "Compiler
Internal Error". Update to the last version (should be 7.1.41).

Warnings "size of symbol ... changed ..." are produced by ifc 7.1 a
the loading stage. These seem to be harmless, but they may cause the
loader to stop, depending on your system configuration. If this happen
and no executable is produced, add the following to LDFLAGS: -Xlinker
--noinhibit-exec. 

Linux distributions using glibc 2.3 or later (such as e.g. RedHat 9) may be
incompatible with ifc 7.0 and 7.1. The incompatibility shows up in the form
of messages "undefined reference to 'errno' " at linking stage. A workaround
is available: see http://newweb.ices.utexas.edu/misc/ctype.c.

\paragraph{Linux PCs with MKL libraries}
On Intel CPUs it is very convenient to use Intel MKL libraries. They can be
also used for AMD CPU, selecting the appropriate machine-optimized
libraries, and also together with non-Intel compilers.
If configure doesn't find MKL, try configure --enable-shared. 
Note that ifort 8 fails to load with MKL v. 5.2 or earlier
versions, because some symbols that are referenced by MKL are missing
There is a fix for this (info from Konstantin Kudin): add libF90.a from ifc
7.1 at the linking stage, as the last library. Note that some combinations of
not-so-recent versions of MKL and ifc may yield a lot of "undefined
references" when statically loaded: use configure --enable-shared, or
remove the -static option in make.sys. Note that pwcond.x works only
with recent versions (v.7 or later) of MKL.

MKL contains optimized FFT routines and a FFTW interface, to be separately
compiled. For 64-bit Intel Core2 processors, they are slightly faster than 
FFTW (MKL v.10, FFTW v.3 fortran interface, reported by P. Giannozzi,
November 2008). 

{\bf Important:} for parallel (MPI) execution on multiprocessor (SMP)
machines, set the environmental variable OMP\_NUM\_THREADS to 1 ! See
the section  ''Running on parallel machines'' for more info on this
and on the difference between MPI and OpenMP parallelization. 

\paragraph{Fun with precompiled libraries}
Since there is no standard fortran compiler for Linux, different
compilers have different ideas about the right way to call external
libraries. As a consequence you may have a mismatch between what your
compiler calls ("symbols") and the actual name of the required 
library call. Use the nm command to determine the name of a library
call, as in the following examples:
\begin{verbatim}
      nm /usr/local/lib/libblas.a | grep -i 'T daxpy'
      nm /usr/local/lib/liblapack.a | grep -i 'T zhegv'
\end{verbatim}
where typical location and name of libraries is assumed. Most precompiled
libraries have lowercase names with one or two underscores (\_) appended.
configure should select the appropriate preprocessing options in make.sys,
but in case of trouble, be aware that:
\begin{itemize}
\item
the Absoft compiler is case-sensitive (like C and unlike other Fortran
compilers) and does not add an underscore to symbol names (note that
if your libraries contain uppercase or mixed case names, you are out
of luck: You must either recompile your own libraries, or change the
\#define's in include/f\_defs.h); 
\item 
both Portland compiler (pgf90) and Intel compiler (ifort/ifc)
are case insensitive and add an underscore to symbol names. 
\end{itemize}
Another potential source of trouble is the incompatibility between I/O
libraries used by different fortran compilers. This manifests itself
under the form of missing routines with strange names (like s\_wsfe, 
do\_fio...) at linking stage. Possible workarounds include
\begin{itemize}
\item
loading the missing routines; it is often sufficient to load -lg2c
(sometimes -lm may also be needed); or 
\item
 (better) to replace the BLAS routine xerbla (it should be the only
  one containing I/O calls) with a recompiled object.  
\end{itemize}
If you choose the latter workaround, locate the library containing
this routine using nm, for instance: 
\begin{verbatim}
            nm /path/to/your/libs/libf77blas.a | grep 'T xerbla'
\end{verbatim}
and replace the object xerbla.o in the library with the one you will
compile. In flib/: 
\begin{verbatim}
            make xerbla.o
            ar rv /path/to/your/libs/libf77blas.a xerbla.o
\end{verbatim}
If nothing works, you may need to recompile the libraries with your fortran
compiler, or to use the internal (slow) copy.

\subsubsection{AMD 32-bit CPUs}
AMD Athlon CPUs can be basically treated like Intel Pentium CPUs. You
can use the Intel compiler and MKL with Pentium-3 optimization.

Konstantin Kudin reports that the best results in terms of performances
are obtained with ATLAS optimized BLAS/LAPACK libraries, using AMD
Core Math Library (ACML) for the missing libraries. ACML can be freely
downloaded from AMD web site. Beware: some versions of ACML -- i.e.
the GCC version with SSE2 -- crash PWscf. The "nosse2" version appears
to be stable. Load first ATLAS, then ACML, then -lg2c, as in the
following example (replace what follows -L with something appropriate
to your configuration):
  \begin{verbatim}
   -L/location/of/fftw/lib/ -lfftw \
   -L/location/of/atlas/lib -lf77blas -llapack -lcblas -latlas \
   -L/location/of/gnu32_nosse2/lib -lacml -lg2c
\end{verbatim}

\subsubsection{64-bit CPUs}
64-bit CPUs like the AMD Opteron and the Intel Itanium are supported and
should work both in 32-bit emulation and in 64-bit mode. Both the Portland
and the Intel compiler (v8.1 EM64T-edition, available via Intel
Premier support) should work. 64-bit executables can address a much
larger memory space, but apparently they are not especially faster
than 32-bit executables. 
eThe Intel compiler has been reported to be more reliable and to produce
faster executables wrt the Portland compiler. You may also try g95 or gfortran.

Beware: the default integer type for 64-bit machine is typically
32-bit long. You should be able to use 64-bit integers as well, 
but it will not give you any advantage and you may run into trouble.

\subsubsection{Linux PC clusters with MPI}
PC clusters running some version of MPI are a very popular
computational platform nowadays. Quantum ESPRESSO is known to work
with at least two of the major MPI implementations (MPICH, LAM-MPI),
plus with the newer MPICH2 and OpenMPI implementation. The number of
possible configurations, in terms of type and version of the MPI
libraries, kernels, system libraries, compilers, is very
large. Quantum ESPRESSO compiles and works on all non-buggy, properly
configured hardware and software combinations. You may have to
recompile MPI libraries: not all MPI installations contain support for
the fortran-90 compiler of your choice (or for any fortran-90 compiler
at all!).  See Axel Kohlmeyer's web site for precompiled versions of
the MPI libraries.  Very useful step-by-step instructions can be found
in the following post by  Javier Antonio Montoya:\\
http://www.democritos.it/pipermail/pw\_forum/2008April/008818.htm . 

If Quantum ESPRESSO does not work for some reason on a PC cluster,
try first if it works in serial execution. A frequent problem with parallel
execution is that Quantum ESPRESSO does not read from standard input,
due to the configuration of MPI libraries: see section 
''Running on parallel machines'' and Axel Kohlmeyer's web site for
more info. 

If you are dissatisfied with the performances in parallel execution,
read the section on ''Parallelization issues''. See also the following
post from Axel Kohlmeyer:\\
http://www.democritos.it/pipermail/pw\_forum/2008-April/008796.html

\subsubsection{Intel Mac OS X}

Newer Mac OS-X machines with Intel CPUs are supported by configure,
with gcc4+g95, gfortran, and the Intel compiler ifort with MKL libraries.

\paragraph{Intel Mac OS X with ifort}

"Uninstall darwin ports, fink and developer tools. The presence of all of
those at the same time generates many spooky events in the compilation
procedure.  I installed just the developer tools from apple, the intel
fortran compiler and everything went on great" (Info by Riccardo Sabatini, 
Nov. 2007)

\paragraph{Intel Mac OS X 10.4 and 10.5 with g95 and gfortran}

The stable and unstable versions of g95 are known to work. Recent
gfortran versions also work, but they may require an updated version
of Developer Tools (XCode 2.4.1 or 2.5), that can be downloaded from
Apple. Some tests fails with mysterious errors, that disappear if
fortran BLAS are linked instead of system Atlas libraries. Use: 
\begin{verbatim}
   BLAS_LIBS      = ../flib/blas.a -latlas
\end{verbatim}
(Info by Paolo Giannozzi, jan.2008)

\section{Running on parallel machines}

Parallel execution is strongly system- and installation-dependent. Typically
one has to specify:
\begin{enumerate}
\item a launcher program, such as poe, mpirun, mpiexec, with or
  without  appropriate options 
\item the number of processors, typically as an option to the launcher
  program,  but in some cases to be specified after the program to be
  executed; 
\item the program to be executed, with the proper path if needed: for
  instance, pw.x, or ./pw.x, or \$HOME/bin/pw.x, or whatever applies; 
\item other Quantum ESPRESSO specific parallelization options, to be
  read and interpreted by the running code: 
\begin{itemize}
\item the number of ``pools'' into which processors are to be grouped
  (pw.x only);
\item the number of ``task groups'' into which processors are to be
  grouped;
\item the number of ``images'' used by NEB calculations;
\item the number of processors performing iterative diagonalization
  (for pw.x) or orthonormalization (for cp.x).
\end{itemize}
\end{enumerate}
Items 1) and 2) are machine- and installation-dependent, and may be 
different for interactive and batch execution. Note that large
parallel machines are  often  configured so as to disallow interactive
execution: if in doubt, ask your system administrator.
Item 3) also depend on your specific configuration (shell, execution
path, etc). 
Item 4) is optional: see section ''Understanding Parallelism''
for the meaning of the various options.

For illustration, here is how to run pw.x on 16 processors partitioned into
8 pools (2 processors each), for several typical cases. For convenience, we
also give the corresponding values of PARA\_ PREFIX, PARA\_POSTFIX to
be used in running the examples distributed with Quantum ESPRESSO (see
section ''Run examples''.

IBM SP machines, batch:
\begin{verbatim}
   pw.x -npool 8 < input
   PARA_PREFIX="", PARA_POSTFIX="-npool 8"
\end{verbatim}
This should also work interactively, with environment variables NPROC
set to 16, MP HOSTFILE set to the file containing a list of processors.

IBM SP machines, interactive, using poe:
\begin{verbatim}
   poe pw.x -procs 16 -npool 8 < input
   PARA_PREFIX="poe", PARA_POSTFIX="-procs 16 -npool 8"
\end{verbatim}
PC clusters using mpiexec:
\begin{verbatim}
   mpiexec -n 16 pw.x -npool 8 < input
   PARA_PREFIX="mpiexec -n 16", PARA_POSTFIX="-npool 8"
\end{verbatim}
SGI Altix and PC clusters using mpirun:
\begin{verbatim}   mpirun -np 16 pw.x -npool 8 < input
   PARA_PREFIX="mpirun -np 16", PARA_POSTFIX="-npool 8"
\end{verbatim}
IBM BlueGene using mpirun:
 \begin{verbatim}
  mpirun -np 16 -exe /path/to/executable/pw.x -args "-npool 8" \
    -in /path/to/input -cwd /path/to/work/directory
\end{verbatim}

\subsection{Understanding Parallelism in Quantum ESPRESSO}

Quantum ESPRESSO uses MPI parallelization.
Data structures are distributed across processors organized in a hierarchy
of groups, which are identified by different MPI communicators level.
The groups hierarchy is as follow:
\begin{verbatim}
  world _ images _ pools _ task groups
                \_ ortho groups
\end{verbatim}
{\bf world}: is the group of all processors (MPI\_COMM\_WORLD).

{\bf images}: Processors can then be divided into different "images",
corresponding to a point in configuration space (i.e. to
a different set of atomic positions). Such partitioning 
is used when performing Nudged Elastic band (NEB), Meta-dynamics 
and Laio-Parrinello simulations.

{\bf pools}: When k-point sampling is used, each image group can be 
subpartitioned into "pools", and k-points can distributed to pools.
Within each pool, reciprocal space basis set (plane waves)
and real-space grids are distributed across processors.
This is usually referred to as "plane-wave parallelization".
All linear-algebra operations on array of  plane waves / 
real-space grids are automatically and effectively parallelized.
3D FFT is used to transform electronic wave functions from
reciprocal to real space and vice versa. The 3D FFT is
parallelized by distributing planes of the 3D grid in real
space to processors (in reciprocal space, it is columns of
G-vectors that are distributed to processors). 

{\bf task groups}: 
In order to allow good parallelization of the 3D FFT when 
the number of processors exceeds the number of FFT planes,
data can be redistributed to "task groups" so that each group 
can process several wavefunctions at the same time.

{\bf ortho group}:
A further level of parallelization, independent on
plane-wave (pool) parallelization, is the parallelization of
subspace diagonalization (pw.x) or iterative orthonormalization
(cp.x). Both operations required the diagonalization of 
arrays whose dimension is the number of Kohn-Sham states
(or a small multiple). All such arrays are distributed block-like
across the "ortho group", a subgroup of the pool of processors,
organized in a square 2D grid. The diagonalization is then performed
in parallel using standard linear algebra operations. 
(This diagonalization is used by, but should not be confused with,
the iterative Davidson algorithm).

{\bf Communications}:
Images and pools are loosely coupled and processors communicate
between different images and pools only once in a while, whereas
processors within each pool are tightly coupled and communications
are significant. This means that Gigabit ethernet (typical for
cheap PC clusters) is ok up to 4-8 processors per pool, but {\em fast}
communication hardware (e.g. Mirynet or comparable) is absolutely 
needed beyond 8 processors per pool.

{\bf Choosing parameters}:
To control the number of images, pools and task groups,
command line switch: -nimage -npools -ntg can be used.
The dimension of the ortho group is set to the largest
value compatible with the number of processors and with
the number of electronic states. The user can choose a smaller
value using the command line switch -ndiag (pw.x) or -northo (cp.x) .
As an example consider the following command line:
\begin{verbatim}
mpirun -np 4096 ./pw.x -nimage 8 -npool 2 -ntg 8 -ndiag 144 -input my.input
\end{verbatim}
This execute the PWscf code on 4096 processors, to simulate a system
with 8 images, each of which is distributed across 512 processors.
K-points are distributed across 2 pools of 256 processors each, 
3D FFT is performed using 8 task groups (64 processors each, so
the 3D real-space grid is cut into 64 slices), and the diagonalization
of the subspace Hamiltonian is distributed to a square grid of 144
processors (12x12).

Default values are: -nimage 1 -npool 1 -ntg 1 ; ndiag is chosen
by the code as the fastest $n^2$ (n integer) that fits into the size
of each pool.

\paragraph{Massively parallel calculations}
For very large jobs (i.e. O(1000) atoms or so) or for very long jobs
to be run on massively parallel  machines (e.g. IBM BlueGene) it is
crucial to use in an effective way both the "task group" and the
"ortho group" parallelization. Without a judicious choice of
parameters, large jobs will find a stumbling block in either memory or 
CPU requirements. In particular, the "ortho group" parallelization is
used in the diagonalization  of matrices in the subspace of Kohn-Sham
states (whose dimension is as a strict minumum equal to the number of
occupied states). These are stored as block-distributed matrixes
(distributed across processors) and diagonalized using custom-taylored
diagonalization algorithms that work on block-distributed matrixes.

Since v.4.1, Scalapack can be used to diagonalize block distributed
matrixes, yielding better speed-up than the default algorithms for
large ( $ > 1000$ ) matroces, when using a large number of processors 
( $> 512$ ). If you want to test scalapack  you have to compile adding
-D\_\_SCALAPACK to DFLAGS in make.sys and you have to  
modify the LAPACK\_LIBS variable like in the following (works on
CINECA BCX machine): 
\begin{verbatim}
SCALAPACK_LIBS = \
/cineca/prod/libraries/SCALAPACK/1.8.0/openmpi--1.2.5--intel--10.1/libscalapack.a
BLACS_LIBS     = \
/cineca/prod/libraries/BLACS/1.1/openmpi--1.2.5--intel--10.1/libblacs.a 
BLACS_INI      = \
/cineca/prod/libraries/BLACS/1.1/openmpi--1.2.5--intel--10.1/libblacsF77init.a
LAPACK_LIBS = $(SCALAPACK_LIBS) $(BLACS_LIBS) $(BLACS_INI) $(BLACS_LIBS) \
                   /cineca/prod/acml/4.1.0/ifort64/lib/libacml.a
\end{verbatim}
(info by Carlo Cavazzoni, Oct. 2008)

\subsection{Tricks and problems}

\paragraph{Trouble with MKL and OpenMP parallelization}
Quantum ESPRESSO uses a parallelization paradigm based on message-passing:
a copy of the executable runs on each CPU, each copy living in a different
world, communicating with other copies via calls to MPI
(Message-Passing Interface) libraries. OpenMP is a different
parallelization paradigm: a single executable spawn subprocesses
(threads) that perform some specific tasks. OpenMP can be implemented
via compiler directives or else via OpenMP-aware libraries such as 
ESSL or MKL. It is typically convenient for SMP (multiprocessor)
machines, with one CPU running the master process and threads started
on the other CPUs. Presently (nov. 2008)  MPI and OpenMP parallelization  
{\em must not be active at the same time}. If you notice very bad parallel 
performances, OpenMP and MPI may be conflicting for the same CPUs. This is 
a frequent problem with MKL, that by default perform OpenMP
autoparallelization. 

When running in parallel with MPI (using "mpirun" for instance) on
multiprocessor  machines, you should set the environmental variable
OMP\_NUM\_THREADS to 1. This is {\em crucial} if your code is linked
with recent versions of MKL.  
Note that if for some reason the correct setting  of variable
OMP\_NUM\_THREADS  
does not propagate to all processors, you may equally run into trouble. 
Lorenzo Paulatto (Nov. 2008) suggests to use the "-x" option to "mpirun" to 
propagate OMP\_NUM\_THREADS to all processors.
Axel Kohlmeyer suggests the following (April 2008): 
"(I've) found that Intel is now turning on multithreading without any
warning and that is for example why their FFT seems faster than
FFTW. For serial and OpenMP based runs this makes no difference (in
fact the multi-threaded FFT helps), but if you run MPI locally, you
actually lose performance. Also if you use the 'numactl' tool on linux
to bind a job to a specific cpu core, MKL will still try to use all
available cores (and slow down badly). The cleanest way of avoiding
this mess is to either link with
\begin{verbatim}
-lmkl_intel_lp64 -lmkl_sequential -lmkl_core (on 64-bit: x86_64, ia64)
-lmkl_intel -lmkl_sequential -lmkl_core (on 32-bit, i.e. ia32 )
\end{verbatim}
or edit the libmkl\_'platform'.a file (I'm using now a file libmkl10.a with:
\begin{verbatim}
  GROUP (libmkl_intel_lp64.a libmkl_sequential.a libmkl_core.a)
\end{verbatim}
It works like a charm".

\paragraph{Trouble with compilers and MPI libraries}
Many users of Quantum ESPRESSO, in particular those working on PC clusters,
have to rely on themselves (or on less-than-adequate system managers) for 
the correct configuration of software for parallel execution. Mysterious and
irreproducible crashes in parallel execution are sometimes due to bugs
in Quantum ESPRESSO, but more often than not are a consequence of buggy
compilers or of buggy or miscompiled MPI libraries. Very useful step-by-step 
instructions to compile and install MPI libraries
can be found in the following post by Javier Antonio Montoya:\\
http://www.democritos.it/pipermail/pw\_forum/2008-April/008818.htm .

On a Xeon quadriprocessor cluster, erratic crashes in parallel
execution have been reported, apparently correlated with ifort 10.1
(info by Nathalie Vast and Jelena Sjakste, May 2008).

\paragraph{Understanding parallel I/O}
In parallel exeution, each processor has its own slice of wavefunctions, 
to be written to temporary files during the calculation. The way wavefunctions 
are written by pw.x is governed by variable wf\_collect, in namelist control. 
If wf\_collect=.true., the final wavefunctions are collected into a single 
directory, written by a single processor, whose format is independent on 
the number of processors. If wf\_collect=.false. (default) each processor
writes its own slice of the final 
wavefunctions to disk in the internal format used by PWscf. 

The former case requires more
disk I/O and disk space, but produces portable data files; the latter case
requires less I/O and disk space, but the data so produced can be read only
by a job running on the same number of processors and pools, and if
all files are on a file system that is visible to all processors
(i.e., you cannot use local scratch directories: there is presently no
way to ensure that the distribution of processes on processors will
follow the same pattern for different jobs).

cp.x instead always collects the final wavefunctions into a single directory.
Files written by pw.x can be read by cp.x only if wf\_collect=.true. (and if
produced for k=0 case). 

With the new file format (v.3.1 and later) all data (except 
wavefunctions in pw.x if wf\_collect=.false.) is written to and read from
a single directory outdir/prefix.save. A copy of pseudopotential files
is also written there. If some processor cannot access outdir/prefix.save,
it reads the pseudopotential files from the pseudopotential directory
specified in input data. Unpredictable results may follow if those files
are not the same as those in the data directory!

Avoid I/O to network-mounted disks (via NFS) as much as you can! 
Ideally the scratch directory (ESPRESSO\_TMPDIR) should be a modern 
Parallel File System. If you do not have any, you can use local
scratch disks (i.e. each node is physically connected to a disk
and writes to it) but you may run into trouble anyway if you 
need to access your files that are scattered in an unpredictable
way across disks residing on different nodes.

You can use option "disk\_io='minimal'", or even 'none', if you run
into trouble (or angry system managers) with eccessive I/O with pw.x. 
The code will store wavefunctions into RAM during the calculation.
Note however that this will increase your memory usage and may limit 
or prevent restarting from interrupted runs.

\paragraph{Trouble with input files}
Some implementations of the MPI library have problems with input 
redirection in parallel. This typically shows up under the form of
mysterious errors when reading data. If this happens, use the option 
-in (or -inp or -input), followed by the input file name. 
Example:
\begin{verbatim}
   pw.x -in inputfile npool 4 > outputfile
\end{verbatim} 
Of course the 
input file must be accessible by the processor that must read it
(only one processor reads the input file and subsequently broadcasts
its contents to all other processors).

Apparently the LSF implementation of MPI libraries manages to ignore or to
confuse even the -in/inp/input mechanism that is present in all
Quantum ESPRESSO codes. In this case, use the -i option of mpirun.lsf
to provide an input 
file.

\paragraph{Cray XT3}
On the cray xt3 there is a special hack to keep files in
memory instead of writing them without changes to the code.
You have to do a: 
module load iobuf
before compiling and then add liobuf at link time.
If you run a job you set the environment variable 
IOBUF\_PARAMS to proper numbers and you can gain a lot.
Here is one example:
\begin{verbatim}
env IOBUF_PARAMS='*.wfc*:noflush:count=1:size=15M:verbose,\
*.dat:count=2:size=50M:lazyflush:lazyclose:verbose,\
*.UPF*.xml:count=8:size=8M:verbose' pbsyod =\
\~{}/pwscf/pwscfcvs/bin/pw.x npool 4 in si64pw2x2x2.inp > & \
si64pw2x2x232moreiobuf.out &
\end{verbatim}
This will ignore all flushes on the *wfc* (scratch files) using a
single i/o buffer large enough to contain the whole file ($\sim 12$ Mb here).
this way they are actually never(!) written to disk.
The *.dat files are part of the restart, so needed, but you can be
'lazy' since they are writeonly. .xml files have a lot of accesses
(due to iotk), but with a few rather small buffers, this can be
handled as well. You have to pay attention not to make the buffers
too large, if the code needs a lot of memory, too and in this example
there is a lot of room for improvement. After you have tuned those
parameters, you can remove the 'verboses' and enjoy the fast execution.
Apart from the i/o issues the cray xt3 is a really nice and fast machine.
(Info by Axel Kohlmeyer, maybe obsolete)

\end{document}
