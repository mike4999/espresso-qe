\documentclass[12pt,a4paper]{article}
 \def\version{2.1}

 \usepackage{epsfig}
 %\usepackage{html}
  \def\htmladdnormallink#1#2{#1}

\begin{document} 

% front page, enlarge to let table of contents fit within
\enlargethispage*{0.5cm}
\thispagestyle{empty}

% PWscf and Democritos logos, raise the latter to align
\begin{center}
  \epsfig{figure=pwscf,width=4cm}\hfill%
  \raisebox{0.5cm}{\epsfig{figure=democritos,width=8cm}}
  \\
  % title
  \huge User's Guide for PWscf v.\version
\end{center}

\tableofcontents

% end of front page
\clearpage

\section{Introduction}

This manual covers the installation and usage of PWscf (Plane-Wave
Self-Consistent Field): a set of programs for electronic structure
calculations within Density-Functional Theory and Density-Functional
Perturbation Theory, using a Plane-Wave basis set and pseudopotentials.
PWscf is a component of the ESPRESSO (opEn-Source Package for Research
in Electronic Structure, Simulation, and Optimization) suite of codes,
including, in addition to PWscf, the Car-Parrinello codes CP and FPMD.

Other sources of documentation can be found in:
\begin{quote}
  the various \texttt{README} files found in the distribution;\\
  the \texttt{Doc/} directory; \\
  the PWscf \htmladdnormallink{web site}{http://www.pwscf.org/};\\
  the mailing list \texttt{pw\_forum@pwscf.org}: see the PWscf web
      site for instructions on how to register and how to search the 
      archives of the mailing list.
\end{itemize}

PWscf can currently perform the following kind of calculations:

\begin{itemize}
  \item ground-state energy and one-electron (Kohn-Sham) orbitals
  \item atomic forces, stresses, and structural optimization
  \item molecular dynamics on the ground-state Born-Oppenheimer surface
  \item variable-cell molecular dynamics
  \item phonon frequencies and eigenvectors at a generic wave vector
  \item effective charges and dielectric tensors
  \item electron-phonon interaction coefficients for metals
  \item interatomic force constants in real space
  \item third-order anharmonic phonon lifetimes
  \item Infrared and Raman (nonresonant) cross section
  \item macroscopic polarization via Berry Phase
  \item Nudged Elastic Band (NEB) for energy barriers and reaction paths
\end{itemize}

All of the above work for both insulators and metals, in any crystal
structure, for many exchange-correlation functionals (including spin
polarization), for both norm-conserving (Hamann-Schl\"uter-Chiang)
pseudopotentials in separable form, and --- with very few exceptions
--- for Ultrasoft (Vanderbilt) pseudopotentials.
Non-colinear magnetism is also implemented, although at an
experimental stage.
Various postprocessing programs are available.
The PWscf codes work on many different types of machines, including
parallel machines using Message Passing Interface (MPI).

Starting from version 2.0, a Graphical User Interface (GUI) is included
in the package. The GUI may be used in producing input data files needed
to run PWscf.

\subsection{People}

The PWscf package was originally developed by
Stefano Baroni\\
\htmladdnormallink{(\texttt{baroni@sissa.it})}%
{mailto:baroni@sissa.it},
Stefano de Gironcoli
\htmladdnormallink{(\texttt{degironc@sissa.it})}%
{mailto:degironc@sissa.it},
Andrea Dal Corso
\htmladdnormallink{(\texttt{dalcorso@sissa.it})}%
{mailto:dalcorso@sissa.it}
(SISSA, Trieste)
and Paolo Giannozzi\\
\htmladdnormallink{(\texttt{giannozz@nest.sns.it})}%
{mailto:giannozz@nest.sns.it} 
(Scuola Normale Superiore, Pisa)
who are the copyleft holders.

The maintenance and further development of the ESPRESSO code is
promoted by the
\htmladdnormallink{DEMOCRITOS National Simulation Center}%
{http://www.democritos.it}
of the Italian INFM
(\texttt{http://www.democritos.it/scientific.php}),
under the coordination of Paolo Giannozzi, with the strong support of
the CINECA National Supercomputing Center in Bologna, under the
responsibility of Carlo Cavazzoni
\htmladdnormallink{(\texttt{c.cavazzoni@cineca.it})}%
{mailto:c.cavazzoni@cineca.it}.
\hyphenation{ge-rar-do}
Currently active developers include Gerardo Ballabio (CINECA), 
Gernot Deinzer, Stefano Fabris, Adriano Mosca Conte, Carlo Sbraccia (SISSA), 
Anton Kokalj (Jo\v{z}ef Stefan Institute, Ljubljana).

Several other people, mostly but not exclusively former SISSA students, 
contribute or have contributed to it. Among them we mention:
\begin{quote}
\hyphenation{laz-ze-ri}
Dario Alf\`e, Francesco Antoniella, Mauro Boero, Claudia Bungaro,
Paolo Cazzato, Gabriele Cipriani, Matteo Cococcioni, Alberto Debernardi, 
Guido Fratesi, Ralph Gebauer, Michele Lazzeri, Kurt M\"ader,
Nicola Marzari, Francesco Mauri, Pasquale Pavone, Mickael Profeta,
Guido Roma, Kurt Stokbro, Renata Wentzcovitch.
\end{quote}

\hyphenation{a-les-sio}
The Berry phase calculation was implemented by Oswaldo Dieguez, 
Alessio Filippetti, Lixin He, Jeff Neaton, in David Vanderbilt's 
group at Rutgers University (Piscataway, NJ).

The GUI for PWscf was written by Anton Kokalj.

This manual was (mostly) written by Paolo Giannozzi.

\subsection{Terms of Use}

PWscf is released under the GNU 
\htmladdnormallink{General Public License}%
{http://www.pwscf.org/License.txt}.

We shall greatly appreciate if scientific work done using this code
will contain an explicit acknowledgment and a reference to the PWscf
web page.
Our preferred form for the acknowledgment is the following:

\begin{quote}
{\em Acknowledgments}
\par\noindent\dots
Calculations in this work have been done using the PWscf package
[ref]\dots
\par\noindent
{\em Bibliography}
\par\noindent
[ref] S. Baroni, A. Dal Corso, S. de Gironcoli, and P. Giannozzi,
\texttt{http://www.pwscf.org/}.
\end{quote}

All trademarks belong to their respective owners.

\clearpage

\section{Installation}

The PWscf package can be downloaded from the
\htmladdnormallink{\texttt{http://www.pwscf.org/}}%
{http://www.pwscf.org/}
site.
Presently, only source files are provided.
Some precompiled executables (binary files) are provided only for the
GUI.
Providing binaries for PWscf would require too much effort and would
work only for a small number of machines anyway.

Uncompress and unpack the code in an empty directory of your choice
that will become the root directory of the distribution.
On Linux machines, you may use:

\texttt{tar xzvf pw.\version.tgz}

\noindent
On other Unix machines:

\texttt{gunzip -c pw.\version.tgz | tar xvf -}

\subsection{Automatic configuration}

An experimental automatic configuration, generated with the GNU
Autoconf package, is available (thanks to Gerardo Ballabio, CINECA).
From the root directory, type:

\texttt{./configure}

\noindent 
The script will examine your hardware and software, generate
dependencies needed by the Makefile's, produce suitable configuration
files \texttt{make.sys} and \texttt{make.rules}.
Presently it is expected to work for Linux PCs, IBM sp machines, SGI
Origin, some HP-Compaq Alpha machines, Cray X1.
See also the \texttt{README.install} file.

If \texttt{configure} completes but it doesn't recognize your 
configuration, you may either edit the \texttt{make.sys} file,
or try to generate a new \texttt{configure} file following 
the instructions in \texttt{README.configure} (please report 
success to the developers). If \texttt{configure} stops with an error
and the nontrivial task of configuring \texttt{configure} scares you,
or if you don't know what to do with the \texttt{make.sys} file, 
you may want to try the manual configuration, described in the next section.

\subsection{Manual configuration}

From the root directory, type:

\texttt{./configure.old} \emph{your-system}

\noindent where \emph{your-system} is one of the following:
\begin{quote}
  \texttt{t3e}:       Cray T3E, Cray compiler\\
  \texttt{sunmpi}:    Sun parallel machines, Sun compiler (+)\\
  \texttt{alphaMPI}:  \hfill HP-Compaq alpha parallel machines,
                      HP-Compaq compiler\\
  \texttt{hpMPI}:     HP PA-RISC parallel machines, HP compiler\\
  \texttt{alpha}:     HP-Compaq alpha workstations, HP-Compaq
                      compiler\\
  \texttt{irix}:      SGI workstations, SGI compiler\\
  \texttt{altix}:     SGI Altix 350/3000 with Linux, Intel compiler\\
  \texttt{pc\_abs}:   Linux PCs, Absoft compiler (+)\\
  \texttt{pc\_pgi}:   Linux PCs, Portland compiler (+)\\
  \texttt{pc\_lahey}: Linux PCs, Lahey compiler (+)\\
  \texttt{cygwin}:    Windows PC, Intel compiler (see below)\\
  \texttt{hp}:        HP PA-RISC workstations, HP compiler\\
  \texttt{ia64}:      HP Itanium workstations, HP compiler\\
  \texttt{sun}:       Sun workstations, Sun compiler (+)\\
  \texttt{fujitsu}:   Fujitsu vector machines, Fujitsu compiler (+)\\
  \texttt{hitachi}:   Hitachi SR8000 \\
  \texttt{sxcross}:   NEC SX-6 (+)
\end{quote}
and has a corresponding file \texttt{Make.}\emph{your-system}
in the \texttt{install/} directory. Note that systems marked with (+) 
in the above list do not support, or did not in the past, or we don't 
know if they do, C-style preprocessing directives.

If your machine/compiler combination is not in the list, choose what
looks like the most similar combination.  You may need to define your
own preprocessing flags and to delve into the code, but usually very
minor changes are needed to port PWscf to an unsupported machine.
See also the {\tt install/README.install} file.

\textbf{Note for HP PA-RISC users:} 
The Makefile for HP PA-RISC workstations and parallel machines is
based on a Makefile contributed by Sergei Lysenkov.
It assumes that you have HP compiler with MLIB libraries installed on
a machine running HP-UX.

\textbf{Note for MS-Windows users:} 
The Makefile for Windows PCs is based on a Makefile written for an
earlier version of PWscf (1.2.0), contributed by Lu Fu-Fa, CCIT,
Taiwan.
Since there have been many changes to the installation procedure, the
provided Makefile --- which has never been tested --- may not work.
You will need the CygWin package (a UNIX environment for PC which runs
in Windows).
The provided Makefiles assumes that you have the Intel compiler with
MKL libraries installed.
Another possibility is to install Linux, either in dual-boot mode, or
running from a CD-ROM. 
You will need to create a partition for Linux and to install a
boot loader (LILO, GRUB).
The latter step is not necessary if you boot from CD-ROM.
The former step could also be avoided in principle (distributions like
Knoppix run directly from the CD-ROM) but for serious use you will
need to have disk access.

\subsection{Adapt to your local configuration}

The file \texttt{make.sys} may still require some tweaking if produced
by the automatic configuration; if produced by manual configuration,
it MUST be edited.
In most cases you will need to specify the name and location of
libraries.
In some cases you may need to change preprocessing options
(\texttt{CPPFLAGS}) and compiler options (\texttt{FFLAGS}).

\textbf{Please Note:}
if you change \texttt{CPPFLAGS} or \texttt{FFLAGS} after the first
compilation, run \texttt{make clean} and recompile everything, unless
you know exactly which routines are affected and how to force their
recompilation.

\subsubsection{Libraries}

PWscf uses the linear algebra BLAS/LAPACK libraries.
A copy of the needed routines is provided with the distribution.
It is however very convenient to use optimized BLAS/LAPACK contained
in vendor-specific mathematical libraries, such as:

\begin{quote}
  \texttt{essl} for IBM\\
  \texttt{complib.sgimath} for SGI Origin\\
  \texttt{scilib} for Cray/T3e\\
  \texttt{sunperf} for Sun\\
  \texttt{MKL} for Intel Linux PCs\\
  \texttt{ACML} for AMD Linux PCs\\
  \texttt{cxml} for HP-Compaq Alphas.
\end{quote}

Otherwise, it is a good idea to use optimized ATLAS libraries instead
of plain BLAS/LAPACK.
If these are not installed or not satisfactory on your system,
download the appropriate binaries from
\htmladdnormallink{\texttt{http://www.netlib.org/}}%
{http://www.netlib.org/}
or download and compile the sources.
This may take some time and effort.

PWscf uses FFT's from the FFTW library
(\htmladdnormallink{\texttt{http://www.fftw.org/}}%
{http://www.fftw.org/})
or from vendor-specific mathematical libraries.
If you use FFTW, you can either compile the copy that comes with the
distribution (this is the default configuration) or link to a
precompiled FFTW library.
In the latter case, follow the instructions in the files
\texttt{Install/Make.*}, and remember that only FFTW v.~$<3$ is
supported: FFTW v.~3 requires changes in the code.

If you want to use external precompiled libraries, you need to
correctly specify their location in the \texttt{make.sys} file.
The \texttt{configure} scripts tries to guess where your libraries
are, but it cannot find libraries in nonstandard places.

\subsubsection{Installation issues}

The main development platforms are IBM SP and Intel/AMD PC with Linux
and Intel compiler.
For other machines, we rely on user's feedback.

\paragraph{All machines}

If you get ``Compiler Internal Error'' or similar messages, try to
lower the optimization level, or to remove optimization, just for the
routine that has problems.
If it doesn't work, or if you experience weird problems, try to
install patches for your version of the compiler (most vendors release
at least a few patches for free), or to upgrade to a more recent
version.

If you get an error in the loading phase that looks like ``ld: file
XYZ.o: unknown (unrecognized, invalid, wrong, missing, \dots) file
type'', or ``While processing relocatable file XYZ.o, no relocatable
objects were found'' (T3E), one of the following things have happened:
\begin{enumerate}
  \item you have leftover object files from a compilation with another 
        compiler: run \texttt{make clean} and recompile.
  \item \texttt{make} does not stop at the first compilation error (it
        happens with some compilers).
        Remove file XYZ.o and look for the compilation error.
\end{enumerate}

If many symbols are missing in the loading phase, you did not specify
the location of all needed libraries (LAPACK, BLAS, FFTW,
machine-specific optimized libraries).
If you did, but symbols are still missing, see below (for Linux PC).

\paragraph{Linux Alphas with Compaq compiler}

If at linking stage you get error messages like: ``undefined reference
to `for\_check\_mult\_overflow64' '' with Compaq/HP fortran compiler
on Linux Alphas, check the following page:\\ 
\texttt{http://linux.iol.unh.edu/linux/fortran/faq/cfal-X1.0.2.html}.

\paragraph{Linux PC}

Since there is no standard compiler for Linux, different compilers
have different ideas about the right way to call external libraries.
As a consequence you may have a mismatch between what your compiler
calls ("symbols") and the actual name of the required library call.
Use the \texttt{nm} command to determine the name of a library call,
as in the following examples:

\texttt{nm /usr/local/lib/libblas.a | grep T | grep -i daxpy } 

\texttt{nm /usr/local/lib/liblapack.a | grep T | grep -i zhegv } 

\noindent
where typical location and name of libraries is assumed.
Most precompiled libraries have lowercase names with one or two
underscores (\_) appended.
Knowing that:
\begin{itemize}
  \item the Absoft compiler is case-sensitive (like C and unlike
        other Fortran compilers) and does not add an underscore
        to symbol names;
  \item both Portland compiler (pgf90) and Intel compiler (ifort/ifc)
        are case insensitive and add an underscore to symbol names;
\end{itemize}
you can select the appropriate preprocessing options in {\tt make.sys}.

If you have precompiled lapack libraries, you may need to add
\texttt{-lg2c} or \texttt{-lm} or both.

\textbf{Note for Absoft compiler}: 
If your libraries contain uppercase or mixed case names, you are out
of luck.
You must either recompile your own libraries, or change the
\texttt{\#define}'s in \texttt{include/f\_defs.h}.

\paragraph{Linux PCs with Portland Group compiler (pgf90)}

PWscf does not work reliably, or not at all, with some versions of the
Portland Group compiler.
In particular, with some versions PWscf works only for small systems,
but not for larger systems.
We think that this is a compiler bug.
Use the latest version of each release of the compiler, with patches
if available: see the Portland Group web site,\\
\texttt{http://www.pgroup.com/faq/install.htm\#release\_info}.

\paragraph{Linux PCs (Pentium) with Intel compiler (ifort, formerly
ifc)}

If \texttt{configure} doesn't find the compiler, or if you get ``Error
loading shared libraries...'' at run time, you have forgotten to
execute the script that sets up the correct path and library path.
Unless your system manager has done this for you, you should execute
the appropriate script --- located in the directory containing the
compiler executable --- in your initialization files.
Consult the documentation provided by Intel. 

Note that each major release of the Intel compiler differs a lot from
the previous one.
Do not mix executables from different releases: they are incompatible.

Some releases of Intel compiler v.~7 and 8 yield ``Compiler Internal 
Error''.
Update to the very latest versions (presently 7.1.41 and 8.0.046, 
respectively), available via Intel Premier support (registration free
of charge for Linux): 
\texttt{http://developer.intel.com/software/products/support/\#premier}.\\
Also note that \texttt{pwcond.x} does not work with some (but not all)
releases of Intel compiler v.~7 and 8, for no apparent good reason.

A warning ``size of symbol ... changed ...'' is produced by ifc 7.1 at
the loading stage.
This seems to be harmless, but it may cause the loader to stop,
depending on your system configuration.
If this happens and no executable is produced, add the following to
\texttt{LDFLAGS}: \texttt{-Xlinker --noinhibit-exec}.

Intel compiler v.~7 and later use a different method to locate
where modules are with respect to v.~$< 7$.
In \texttt{make.sys}, choose the appropriate line
\texttt{MODULEFLAG=...} (obsolete with automatic configuration).

On Intel CPUs, it is very convenient to use Intel MKL libraries.
Note that MKL also contains optimized FFT routines, but they are
presently not supported: use FFTW instead. Also note that Intel 
compiler v.~8 fails to load with MKL v.~5.2 or earlier versions,
because some symbols that are referenced by MKL are missing. There
is a fix for this (info from Konstantin Kudin): add libF90.a from 
ifc 7.1 at the linking stage, as the last library (info from Konstantin Kudin).

The I/O libraries used by the Intel compiler ifc are incompatible with
those called by most precompiled BLAS/LAPACK libraries (including
ATLAS): you get error messages at linking stage.
The workaround is to recompile BLAS/LAPACK with ifc, or (better) to
replace the BLAS routine \texttt{xerbla} and LAPACK routine
\texttt{dlamch} (the only two containing I/O calls) with recompiled
objects:
\begin{quote}
  \texttt{ifc -c xerbla.f}\\
  \texttt{ifc -O0 -c dlamch.f}
\end{quote}
(do not forget \texttt{-O0} --- \texttt{dlamch.f} \emph{must} be
compiled without optimization) and replace them into the library, as
in the following example:
\begin{quote}
  \texttt{ar rv libatlas.a xerbla.o dlamch.o}
\end{quote}
(assuming that the library and the two object files are in the same
directory).
Note that if you have a PC cluster running MPI, you may have to
recompile MPI libraries as well, using the Intel C compiler (icc).

Linux distributions using glibc 2.3 or later (such as e.g. RedHat 9)
may be incompatible with ifc 7.0 and 7.1.
The incompatibility shows up in the form of messages ``undefined
reference to `errno' '' at linking stage.
A workaround is available: see
\texttt{http://newweb.ices.utexas.edu/misc/ctype.c}.

\paragraph{AMD CPUs, Intel Itanium}

AMD Athlon CPUs can be basically treated like Intel Pentium CPUs.
You can use the Intel compiler and MKL with Pentium-3 optimization.
The best results have been reported with ATLAS optimized BLAS/LAPACK
libraries, and AMD Core Math Library (ACML) for the missing libraries:
load first ATLAS, then ACML, then \texttt{-lg2c}.
ACML can be freely downloaded from AMD web site.
This info was contributed by Konstantin Kudin.

AMD Opteron CPUs should run in 64-bit mode with the Portland compiler:
\texttt{-D\_\_LINUX64} is needed among the preprocessing flags.
It is also known to run with the Intel compiler in 32-bit emulation.
64-bit executables can address a much larger memory space, but
apparently they are not especially faster than 32-bit executables.
There are rumors that the just released Intel compiler v.~8.1
available via Intel Premier support, works fine with Opterons in 
64-bit mode.

Intel Itanium is untested, but it should work, provided that
\texttt{-D\_\_LINUX64} is used (if 64-bit execution is desired).

\paragraph{T3E}

The following workaround is needed: in files \texttt{PW/bp\_zgefa.f}
and \texttt{PW/bp\_zgedi.f}, replace all occurrences of
\texttt{zscal}, \texttt{zaxpy}, \texttt{zswap}, \texttt{izamax} with
\texttt{cscal}, \texttt{caxpy}, \texttt{cswap}, \texttt{icamax}.
Also, in \texttt{PP/dist.f} you need to comment the call to
\texttt{getarg} and uncomment the call to \texttt{pxfgetarg}.

If you have a T3E with ``benchlib'' installed, you may want to use it
by adding \texttt{-D\_\_BENCHLIB} to preprocessing flags.
If you get errors at loading because symbols \texttt{LPUTP},
\texttt{LGETV}, \texttt{LSETV} are undefined, you either need to link
``benchlib'', or to remove \texttt{-D\_\_BENCHLIB} and recompile
(after a \texttt{make clean}).

\subsection{Compilation}

There are a few adjustable parameters in
\texttt{Modules/parameters.f90}.
The present values will work for most cases.
All other variables are dynamically allocated: you do not need to
recompile your code for a different system.

You can compile the following codes:

\begin{itemize}
  \item
    \texttt{make pw} produces \texttt{PW/pw.x} and
    \texttt{PW/memory.x}.

    \texttt{pw.x} calculates electronic structure, structural
    optimization, molecular dynamics, barriers with NEB.
    \texttt{memory.x} is an auxiliary program that checks the input of
    \texttt{pw.x} for correctness and yields a rough (under-)estimate
    of the required memory.
  \item
    \texttt{make ph} produces \texttt{PH/ph.x}.

    \texttt{ph.x} calculates phonon frequencies and displacement
    patterns, dielectric tensors, effective charges (uses data
    produced by \texttt{pw.x}).
  \item
    \texttt{make d3} produces \texttt{D3/d3.x}

    \texttt{d3.x} calculates anharmonic phonon lifetimes (third-order
    derivatives of the energy), using data produced by \texttt{pw.x}
    and \texttt{ph.x}.
  \item
    \texttt{make gamma} produces \texttt{Gamma/phcg.x}.

    \texttt{phcg.x} is a version of \texttt{ph.x} that calculates
    phonons at $\mathbf{q}=0$ using conjugate-gradient minimization of
    the density functional expanded to second-order.
    Only the $\Gamma$ ($\mathbf{q}=0$) point is used for Brillouin
    zone integration.
    It is faster and takes less memory than \texttt{ph.x}, but does
    not support Ultrasoft pseudopotentials.
  \item
    \texttt{make raman} produces \texttt{Raman/ram.x}.

    \texttt{ra.x} calculates nonresonant Raman tensor coefficients
    (derivatives f the polarizability wrt atomic displacements)
    using the $(2n+1)$ theorem (written by Gernot Deinzer).
  \item
    \texttt{make pp} produces the following postprocessing codes in
    \texttt{PP/}:
    \begin{itemize}
      \item \texttt{pp.x} extracts data from files produced by
            \texttt{pw.x} for further processing
      \item \texttt{bands.x} extracts eigenvalues from files produced
            by \texttt{pw.x} for further processing
      \item \texttt{projwfc.x} calculates projections of wavefunction
            over atomic orbitals, performs L\"owdin population
            analysis and calculates projected density of states.
      \item \texttt{chdens.x} plots data produced by \texttt{pp.x},
            writing them into a format that is suitable for several
            plotting programs
      \item \texttt{plotrho.x} reads the output of \texttt{chdens.x},
            produces PostScript 2-d contour plots
      \item \texttt{plotband.x} reads the output of \texttt{bands.x},
            produces band structure PostScript plots
      \item \texttt{average.x} calculates planar averages of
            potentials
      \item \texttt{voronoy.x} divides the charge density into Voronoy
            polyhedra
      \item \texttt{dos.x} calculates electronic Density of States
            (DOS).
      \item \texttt{pw2wan.x}: interface with a code calculating
            Wannier functions
      \item \texttt{pw2casino.x}: interface with \texttt{CASINO} code
            for Quantum Monte Carlo calculation.
    \end{itemize}
  \item
    \texttt{make tools} produces utility programs, mostly for phonon
    calculations, in \texttt{pwtools/}:
    \begin{itemize}
      \item \texttt{dynmat.x} calculates LO-TO splitting at
            $\mathbf{q}=0$ in insulator, IR cross sections, from the
            dynamical matrix produced by \texttt{ph.x}
      \item \texttt{q2r.x} calculates Interatomic Force Constants ion
            real space from dynamical matrices produced by
            \texttt{ph.x} on a regular \textbf{q}-grid
      \item \texttt{matdyn.x} produces phonon frequencies at a generic
            wave vector using the Interatomic Force Constants
            calculated by \texttt{q2r.x}; may also calculate phonon
            DOS
      \item \texttt{fqha.x} for quasi-harmonic calculations
      \item \texttt{dist.x} calculates distances and angles between
            atoms in a cell, taking into account periodicity
      \item \texttt{ev.x} fits energy-vs-volume data to an equation of
            state
      \item \texttt{kpoints.x} produces lists of k-points
      \item \texttt{pwi2xsf.sh}, \texttt{pwo2xsf.sh} process
            respectively input and output files (not data files!) for
            \texttt{pw.x} and produce an XSF-formatted file suitable
            for plotting with XCrySDen, a powerful crystalline and
            molecular structure visualization program\\
            (\texttt{http://www.xcrysden.org/}).
            BEWARE: the \texttt{pwi2xsf.sh} shell script requires the
            \texttt{pwi2xsf.x} executables to be located somewhere in
            your \texttt{\$PATH}.
      \item \texttt{band\_plot.x}: undocumented
      \item \texttt{bs.awk}, \texttt{mv.awk} are scripts that process
            the output of \texttt{pw.x} (not data files!).
            Usage: \\
            \texttt{awk -f bs.awk < my-pw-file > myfile.bs } (basic
            file)\\
            \texttt{awk -f mv.awk < my-pw-file > myfile.mv } (movie
            file)\\
            The files so produced are suitable for use with
            \texttt{xbs}, a very simple X-windows utility to display
            molecules, available at:\\
            \texttt{http://www.ccl.net/cca/software/X-WINDOW/xbsa/\\%
                    README.shtml}.
    \end{itemize}
  \item
    \texttt{make nc} produces \texttt{PWNC/pw.x}, the version of
    \texttt{pw.x} that works with noncolinear magnetism
    (experimental).
  \item
    \texttt{make pwcond} produces \texttt{PWCOND/pwcond.x}, for
    ballistic conductance calculations (experimental).
  \item
    \texttt{make pwall} produces all of the above.
  \item
    \texttt{make upf} produces utilities for pseudopotential
    conversion in directory \texttt{upftools/} (see
    ``Pseudopotential'' section).
  \item
    \texttt{make links} produces links to all executables in directory
    \texttt{bin/}.
  \item
    \texttt{make all} produces all of the above, plus the CP and FPMD
    codes (if present).
\end{itemize}
For the setup of the GUI, refer to the
\texttt{PWgui-}\emph{X.Y.Z}\texttt{/INSTALL} file, where \emph{X.Y.Z}
stands for the version number.
If you are using the CVS-sources, then see the \texttt{GUI/README}
file instead.

\clearpage

\section{Pseudopotentials}

Currently PWSCF supports both Ultrasoft (US) Vanderbilt
pseudopotentials (PPs) and Norm-Conserving (NC)
Hamann-Schl\"uter-Chiang PPs in separable Kleinman-Bylander form.
Note however that calculation of third-order derivatives is not (yet)
implemented with US PPs.

PWSCF uses a
\htmladdnormallink{\textbf{unified pseudopotential format}}%
{http://www.pwscf.org/format.htm}
(UPF) for all types of PPs, but still accepts a number of
\htmladdnormallink{other formats}{http://www.pwscf.org/oldformat.htm}:
\begin{enumerate}
  \item the ``old PWSCF'' format for NC PPs
  \item the ``Vanderbilt'' format (formatted, not binary) for NC and
        US PPs
  \item the ``new PWSCF'' format for NC and US PPs.
\end{enumerate}

PPs for selected elements can be downloaded from the PWSCF 
\htmladdnormallink{PP page}{http://www.pwscf.org/pseudo.html}.
If you do not find there the PP you need (because there is no PP for
the atom you need or you need a different exchange-correlation
functional or a different core-valence partition or for whatever
reason may apply), it may be taken, if available, from published
tables, such as e.g.:
\begin{itemize}
  \item G.B. Bachelet, D.R. Hamann and M. Schl\"uter, Phys. Rev. B
        \textbf{26}, 4199 (1982)
  \item X. Gonze, R. Stumpf, and M. Scheffler, Phys. Rev. B
        \textbf{44}, 8503 (1991)
  \item S. Goedecker, M. Teter, and J. Hutter, Phys. Rev. B
        \textbf{54}, 1703 (1996)
\end{itemize}
or otherwise it must be generated.
This is not as daunting a task as most people think.

Since PWscf v.~2.1, a PP generation package is provided with PWscf, in
the directory \texttt{atomic/} (sources) and \texttt{atomic\_doc/}
(documentation, tests and examples). 
The package can generate both NC and US PPs, in various formats that
are directly read by PWscf.
We refer to its documentation for instructions on how to generate PPs
with the \texttt{atomic/} code.

Other PP generation packages are available on-line:

\begin{itemize}
  \item
    \htmladdnormallink{David Vanderbilt's code}%
    {http://www.physics.rutgers.edu/~dhv/uspp/index.html}
    (UltraSoft PPs):\\
    \texttt{http://www.physics.rutgers.edu/\~{}dhv/uspp/index.html}
  \item
    \htmladdnormallink{The Fritz Haber code}%
    {http://www.fhi-berlin.mpg.de/th/fhi98md/fhi98PP}
    (Norm-Conserving PPs):\\
    \texttt{http://www.fhi-berlin.mpg.de/th/fhi98md/fhi98PP}
  \item
    \htmladdnormallink{Jos\'e-Lu{\'\i}s Martins' code}%
    {http://bohr.inesc-mn.pt/~jlm/pseudo.html}
    (Norm-Conserving PPs):\\
    \texttt{http://bohr.inesc-mn.pt/\~{}jlm/pseudo.html}
\end{itemize}

The first three codes produce PPs in UPF format, or in a format that
can be converted to unified format using the utilities of directory
\texttt{upftools/}.

Remember: \emph{always} test the PPs on simple test systems before
proceeding to serious calculations.

\clearpage

\section{Using PWscf}

Input data for the PWscf can be easily produced using the graphical
user interface \texttt{PWgui}, by Anton Kokalj, distributed together
with the PWscf package.
See \texttt{PWgui-X.Y.Z/INSTALL} for more info on this subject (or
\texttt{GUI/README} if you are using CVS sources).

Tests and examples, together with the pseudopotentials used, can be
downloaded from the
\htmladdnormallink{Tests and Examples Page}%
{http://www.pwscf.org/tests.htm}.
See the documentation contained there.
The examples assume that all executables reside in a subdirectory
\texttt{bin/}: use command \texttt{make links} in the root directory
to create links to all executables in \texttt{bin/}.

Note about exchange-correlation: the type of exchange-correlation used
in the calculation is read from PP files.
All PP's must have been generated using the same exchange-correlation. 

In the following, ``Example N'' refers to the contents of directory\\
\texttt{examples/example0N} if N$<10$; \texttt{examples/exampleN}
otherwise.

\subsection{Electronic and ionic structure calculations}

Electronic and ionic structure calculations are performed by program
\texttt{pw.x}.

\subsubsection{Input data}

The input data is organized as several namelists, followed by other
fields introduced by keywords.

The namelists are 
\begin{quote}
  \texttt{\&CONTROL}: general variables controlling the run\\
  \texttt{\&SYSTEM}: structural information on the system under
    investigation\\
  \texttt{\&ELECTRONS}: electronic variables: self-consistency,
    smearing\\
  \texttt{\&IONS} (optional): ionic variables: relaxation,
    dynamics\\
  \texttt{\&CELL} (optional): variable-cell dynamics\\
  \texttt{\&PHONON} (optional): information required to produce
    data for phonon calculations
\end{quote}

Optional namelist may be omitted if the calculation to be performed
does not require them.
This depends on the value of variable \texttt{calculation} in namelist
\texttt{\&CONTROL}.
Most variables in namelists have default values.
Only the following variables in \texttt{\&SYSTEM} must always be
specified:
\begin{quote}
  \texttt{ibrav} (integer): bravais-lattice index\\
  \texttt{celldm} (real, dimension 6): crystallographic constants\\
  \texttt{nat} (integer): number of atoms in the unit cell\\
  \texttt{ntyp} (integer): number of types of atoms in the unit cell\\
  \texttt{ecutwfc} (real): kinetic energy cutoff (Ry) for
    wavefunctions.
\end{quote}

Explanations for the meaning of variables \texttt{ibrav} and
\texttt{celldm} are in file \texttt{INPUT\_PW}.
Please read them carefully.
There is a large number of other variables, having default values,
which may or may not fit your needs.

After the namelists, you have several fields introduced by keywords
with self-explanatory names:

\begin{quote}
  \texttt{ATOMIC\_SPECIES}\\
  \texttt{ATOMIC\_POSITIONS}\\
  \texttt{K\_POINTS}\\
  \texttt{CELL\_PARAMETERS} (optional) \\
  \texttt{CLIMBING\_IMAGES} (optional)
\end{quote}

The keywords may be followed on the same line by an option.
Unknown fields (including some that are specific to CP and FPMD codes)
are ignored by PWscf.
See file \texttt{Doc/INPUT\_PW} for a detailed explanation of the
meaning and format of the various fields.

Note about k points:
The k-point grid can be either automatically generated or manually
provided as a list of k-points and a weight in the Irreducible
Brillouin Zone only of the \emph{Bravais lattice} of the crystal.
The code will generate (unless instructed not to do so: see variable
\texttt{nosym}) all required k-points and weights if the symmetry of
the system is lower than the symmetry of the Bravais lattice.
The automatic generation of k-points follows the convention of
Monkhorst and Pack.

\subsubsection{Typical cases}

We may distinguish the following typical cases for \texttt{pw.x}:

\begin{description}

  \item [single-point (fixed-ion) SCF calculation.]
    Set \texttt{calculation='scf'}.

    Namelists \texttt{\&IONS} and \texttt{\&CELL} need not to be
    present (this is the default).
    See Example 01.

  \item [band structure calculation.]
    First perform a SCF calculation as above; then do a non-SCF
    calculation specifying \texttt{calculation='nscf'}, with the
    desired k-point grid and number \texttt{nbnd} of bands.

    Specify \texttt{nosym=.true.} to avoid generation of additional
    k-points in low symmetry cases.
    Variables \texttt{prefix} and \texttt{outdir}, which determine the
    names of input or output files, should be the same in the two
    runs.
    See Example~01.

  \item [structural optimization.]
    \hyphenation{name-list}
    Specify \texttt{calculation='relax'} and add namelist \texttt{\&IONS}.

    All options for a single SCF calculation apply, plus a few others.
    You may follow a structural optimization with a non-SCF band-structure
    calculation, but do not forget to update the input ionic coordinates.
    See Example 03.

  \item [molecular dynamics.]
    Specify \texttt{calculation='md'} and time step \texttt{dt}.

    Use variable \texttt{ion\_dynamics} in namelist \texttt{\&IONS} for a
    fine-grained control of the kind of dynamics.
    Other options for setting the initial temperature and for
    thermalization using velocity rescaling are available.
    Remember: this is MD on the electronic ground state, not
    Car-Parrinello MD.
    See Example 04.

  \item [polarization via Berry Phase.]
    See Example 10, its \texttt{README}, and the documentation in the
    header of \texttt{PW/bp\_c\_phase.f90}.

  \item [Nudged Elastic Band calculation.]
    \hfill Specify \texttt{calculation='neb'} and add namelist
    \texttt{\&IONS}.

    All options for a single SCF calculation apply, plus a few others.
    In the namelist \texttt{\&IONS} the number of images used to
    discretize the elastic band must be specified.
    All other variables have a default value.
    Coordinates of the initial and final image of the elastic band have to
    be specified in the \texttt{ATOMIC\_POSITIONS} card.
    A detailed description of all input variables is contained in the file
    \texttt{Doc/INPUT\_PW}.
    See also Example 17.

\end{description}

The output data files are written in the directory specified
by variable \texttt{outdir}, with names specified by variable 
\texttt{prefix} (a string that is prepended to all file names,
whose default value is: \texttt{prefix='pwscf'}).

The execution stops if you create a file \texttt{prefix.EXIT} in the
working directory. Note that just killing the process may leave the 
output files in an unusable state.

\subsection{Phonon calculations}

The phonon code \texttt{ph.x} calculates normal modes at a given
\textbf{q}-vector, starting from data files produced by \texttt{pw.x}.

If $\mathbf{q}=0$, the data files can be produced directly by a simple
SCF calculation.
For phonons at a generic \textbf{q}-vector, you need to perform first
a SCF calculation, then a band-structure calculation (see above)
with\\ \texttt{calculation='phonon'}, specifying the \textbf{q}-vector
in variable \texttt{xq} of namelist \texttt{\&PHONON}.

The output data file appear in the directory specified by variables
\texttt{outdir}, with names specified by variable \texttt{prefix}.
After the output file(s) has been produced (do not remove any of the
files, unless you know which are used and which are not), you can run
\texttt{ph.x}.

The first input line of \texttt{ph.x} is a job identifier.
At the second line the namelist \texttt{\&INPUTPH} starts.
The meaning of the variables in the namelist (most of them having a
default value) is described in file \texttt{INPUT\_PH}.
Variables \texttt{outdir} and \texttt{prefix} must be the same as in
the input data of \texttt{pw.x}.
Presently you must also specify \texttt{amass} (real, dimension
\texttt{ntyp}): the atomic mass of each atomic type.

After the namelist you must specify the \textbf{q}-vector of the
phonon mode.
This must be the same \textbf{q}-vector given in the input of
\texttt{pw.x}.

A sample phonon calculation is performed in Example 02.

\subsubsection{Calculation of Interatomic Force Constants in real
space}

First, dynamical matrices are calculated and saved for a suitable
uniform grid of \textbf{q}-vectors.
Only the \textbf{q}-vectors in the Irreducible Brillouin Zone of the
crystal are needed.
If the system is an insulator, effective charges and dielectric tensor
must be calculated (variable \texttt{epsil=.true}) at $\mathbf{q}=0$.

Second, all dynamical matrices are given as input to code
\texttt{q2r.x}.
The $\mathbf{q}=0$ file must be the first in the list.
This produces a file of Interatomic Force Constants in real space, up
to a distance that depends on the size of the grid of
\textbf{q}-vectors.
Program \texttt{matdyn.x} may be used to produce phonon modes and
frequencies at any \textbf{q} using the Interatomic Force Constants
file as input.
Note that if you want to calculate LO-TO splitting and IR cross
sections in insulators at $\mathbf{q}=0$ you should use program
\texttt{dynmat.x} instead.

See Example 06.

\subsubsection{Calculation of electron-phonon interaction
coefficients}

The calculation of electron-phonon coefficients in metals is made
difficult by the slow convergence of the sum at the Fermi energy.
It is convenient to calculate phonons, for each \textbf{q}-vector of a
suitable grid, using a smaller k-point grid, saving the dynamical
matrix and the self-consistent first-order variation of the potential
(variable \texttt{fildvscf}).
Then a non-SCF calculation with a larger k-point grid is performed.
Finally the electron-phonon calculation is performed by specifying
\texttt{elph=.true.}, \texttt{trans=.false.}, and the input files
\texttt{fildvscf}, \texttt{fildyn}.
The electron-phonon coefficients are calculated using several values
of gaussian broadening (see \texttt{PH/elphon.f90}) because this
quickly shows whether results are converged or not with respect to the
k-point grid and Gaussian broadening.

All of the above must be repeated for all desired \textbf{q}-vectors
and the final result is summed over all \textbf{q}-vectors (code not
yet available).

See Example 07.

\subsection{Post Processing}

There are a number of auxiliary codes performing postprocessing tasks
such as plotting, averaging, and so on, on the various quantities
calculated by \texttt{pw.x}.
Such quantities are saved by \texttt{pw.x} into the output data
file(s).

The main postprocessing code \texttt{pp.x} reads data file(s) and may
produce on output either the projection of wavefunctions on atomic
wavefunctions, or another file containing one of the following
quantities:

\begin{quote}
  charge\\
  spin polarization\\
  various potentials\\
  local density of states at $E_F$\\
  local density of electronic entropy\\
  STM images\\
  wavefunction squared\\
  electron localization function\\
  planar averages\\
  integrated local density of states
\end{quote}

See file \texttt{INPUT\_PP} for a detailed description of the input
for code \texttt{pp.x}.

The file(s) produced by \texttt{pp.x} are processed by program
\texttt{chdens.x} for plotting.
The type of plotting (along a line, on a plane, three-dimensional,
polar) and the output format must be specified here.
The output file can be directly read by the free plotting system
Gnuplot (1D or 2D plots), or by code \texttt{plotrho.x} that comes
with PWscf (2D plots), or by advanced plotting software XCrySDen and
gOpenMol (3D plots).
More details on the input data are written in the header of file
\texttt{PP/chdens.f90}.
See Example 05 for a charge density plot.

The postprocessing code \texttt{bands.x} reads data file(s), extracts
eigenvalues, regroups them into bands (the algorithm used to order
bands and to resolve crossings may not work in all circumstances,
though).
The output is written to a file in a simple format that can be
directly read by plotting program \texttt{plotband.x}.
Unpredictable plots may results if \textbf{k}-points are not in
sequence along lines.
See Example 05 for a simple band plot.

The postprocessing code \texttt{projwfc.x} calculates projections of
wavefunction over atomic orbitals.
The atomic wavefunctions are those contained in the pseudopotential
file(s).
The L\"owdin population analysis (similar to Mulliken analysis) is
presently implemented.
The projected DOS (the DOS projected onto atomic orbitals) can also be
calculated.
More details on the input data are found in the header of file
\texttt{PP/projwfc.f90}.
The total electronic DOS is instead calculated by code
\texttt{PP/dos.x}.
See Example 08 for total and projected electronic DOS calculations.

The postprocessing code \texttt{path\_int.x} is intended to be used in
the framework of NEB calculations.
It is a tool to generate a new path (what is actually generated is the
restart file) starting from an old one through interpolation (cubic
splines).
The new path can be discretized with a different number of images
(this is its main purpose), images are equispaced and the
interpolation can be also performed on a subsection of the old path.
The input file needed by \texttt{path\_int.x} can be easily set up
with the help of the self explanatory \texttt{path\_int.sh} shell
script.

\clearpage

\section{Running PWscf on parallel machines}

Parallel execution is strongly system- and installation-dependent.
Typically one has to specify:

\begin{itemize}
  \item a launcher program, such as \texttt{poe}, \texttt{mpirun}, or
        \texttt{mpiexec};
  \item the number of processors, typically as an option to the
        launcher program, but in some cases \emph{after} the program
        to be executed;
  \item the program to be executed, with the proper path if needed:
        for instance, \texttt{pw.x}, or \texttt{./pw.x}, or
        \texttt{\$(HOME)/bin/pw.x}, or whatever applies;
  \item the number of ``pools'' into which processors are to be
        grouped (see Section ``Parallelization Issues'' below for an
        explanation of what a pool is).
\end{itemize}

The last item is optional and is read by the code.
The first and second items are machine- and installation-dependent,
and may be different for interactive and batch execution.
To run the examples, one has to set \texttt{PARA\_PREFIX} and
\texttt{PARA\_POSTFIX} in file
\texttt{examples/environment\_variables} to the proper values.

Let us consider the case of execution of \texttt{pw.x} on 16
processors divided into 8 pools (2 processors each).
Some typical cases:

\begin{itemize}
  \item
    IBM SP machines, batch:\\
    \texttt{pw.x -npool 8 < input}\\
    (\texttt{PARA\_PREFIX=""}, \texttt{PARA\_POSTFIX="-npool 8"}). 
    This might also work interactively, with environment variables
    \texttt{NPROC} set to 16, \texttt{MP\_HOSTFILE} set to the file
    containing a list of processors.
  \item
    IBM SP machines, interactive, using \texttt{poe}:\\
    \texttt{poe pw.x -procs 16 -npool 8 < input}\\
    (\texttt{PARA\_PREFIX="poe"}, \texttt{PARA\_POSTFIX="-procs 16
    -npool 8"}).
  \item
    SGI Origin, PC clusters using \texttt{mpirun}:\\
    \texttt{mpirun -np 16 pw.x -npool 8 < input}\\
    (\texttt{PARA\_PREFIX="mpirun -np 16"},
    \texttt{PARA\_POSTFIX="-npool 8"}).
  \item
    Cray T3E (old):\\
    \texttt{mpprun -n 16 pw.x -npool 8 < input}\\
    (\texttt{PARA\_PREFIX="mpprun -n 16"},
    \texttt{PARA\_POSTFIX="-npool 8"}).
  \item
    PC clusters using \texttt{mpiexec}:\\
    \texttt{mpiexec -n 16 pw.x -npool 8 < input}\\
    (\texttt{PARA\_PREFIX="mpiexec -n 16"},
    \texttt{PARA\_POSTFIX="-npool 8"}).
\end{itemize}

Note that each processor writes its own set of temporary wavefunction
files during the calculation.
If \texttt{wf\_collect=.true.} (in namelist \texttt{control}), the
final result is collected into a single file, whose format is
independent on the number of processors; otherwise, one wavefunction
file per processor is left on the disk.
In the latter case, the files are readable only by a job running on
the same number of processors and pools, and if all files are on a
file system that is visible to all processors (i.e., you cannot use
local scratch directories: there is presently no way to ensure that
the distribution of processes on processors will follow the same
pattern for different jobs).

Some implementations of the MPI library may have problems with
input redirection in parallel.
If this happens, use the option \texttt{-in} (or \texttt{-inp} or
\texttt{-input}), followed by the input file name.
Example: \texttt{pw.x -in input -npool 4 > output}.

Please note that all postprocessing codes \emph{not} reading data
files produced by \texttt{pw.x} --- that is, \texttt{chdens.x},
\texttt{average.x}, \texttt{voronoy.x}, \texttt{dos.x} --- the
plotting codes \texttt{plotrho.x}, \texttt{plotband.x}, and all
executables in \texttt{pwtools/}, should be executed on just one
processor.
Unpredictable results may follow if those codes are run on more than
one processor.

\clearpage

\section{Performance Issues}

\subsection{CPU time requirement}

The following holds for code {\tt pw.x} and for non-US PPs. 
For US PPs there are additional terms to be calculated.
For phonon calculations, each of the $3 N_{at}$ modes requires a CPU
time of the same order of that required by a self-consistent 
calculation in the same system.

The computer time required for the self-consistent solution at fixed
ionic positions, $T_{scf}$, is:
$$
T_{scf} = N_{iter} \cdot T_{iter} + T_{init}
$$
where $N_{iter}=\mathtt{niter}=$ number of self-consistency
iterations, $T_{iter}=$ CPU time for a single iteration,
$T_{sub}=$ initialization time for a single iteration. 
Usually $T_{init} << N_{iter} \cdot T_{iter}$.

The time required for a single self-consistency iteration
$T_{iter}$ is:
$$
T_{iter} = N_k \cdot T_{diag} + T_{rho} + T_{scf}
$$
where $N_k=$ number of k-points, $T_{diag}=$ CPU time per hamiltonian
iterative diagonalization, $T_{rho}=$ CPU time for charge density
calculation, $T_{scf}=$ CPU time for Hartree and exchange-correlation
potential calculation.

The time for a Hamiltonian iterative diagonalization $T_{diag}$ is:
$$
T_{diag} = N_h \cdot T_h + T_{orth} + T_{sub}
$$
where $N_h=$ number of $H\psi$ products needed by iterative
diagonalization, $T_h=$ CPU time per $H\psi$ product, $T_{orth}=$ CPU
time for orthonormalization, $T_{sub}=$ CPU time for subspace
diagonalization.

The time $T_h$ required for a $H\psi$ product is
$$
T_h = a_1 \cdot M \cdot N
      + a_2 \cdot M \cdot N_1 \cdot N_2 \cdot N_3 \cdot
        \log(N_1 \cdot N_2 \cdot N_3)
      + a_3 \cdot M \cdot P \cdot N.
$$
The first term comes from the kinetic term and is usually much smaller
than the others.
The second and third terms come respectively from local and nonlocal
potential.
$a_1$, $a_2$, $a_3$ are prefactors, $M=$ number of valence bands,
$N=$ number of plane waves (basis set dimension), 
$N_1$, $N_2$, $N_3=$ dimensions of the FFT grid for wavefunctions
($N_1 \cdot N_2 \cdot N_3 \sim 8N$), $P=$ number of projectors for PPs
(summed on all atoms, on all values of the angular momentum $l$, and
$m=1,\dots,2l+1$)

The time $T_{orth}$ required by orthonormalization is
$$
      T_{orth}=b_1*M_x^2*N
$$
and the time $T_{sub}$ required by subspace diagonalization is
$$
   T_{sub}=b_2*M_x^3
$$
where $b_1$ and $b_2$ are prefactors, $M_x=$ number of trial
wavefunctions (this will vary between $M$ and a few times $M$,
depending on the algorithm).

The time $T_{rho}$ for the calculation of charge density from
wavefunctions is
$$
T_{rho} = c_1 \cdot M \cdot Nr_1 \cdot Nr_2 \cdot Nr_3 \cdot
          \log(Nr_1 \cdot Nr_2 \cdot Nr_3)
          + c_2 \cdot M \cdot Nr_1 \cdot Nr_2 \cdot Nr_3 + T_{us}
$$
where $c_1$, $c_2$, $c_3$ are prefactors,
$Nr_1$, $Nr_2$, $Nr_3=$ dimensions of the FFT grid for charge density
($Nr_1 \cdot Nr_2 \cdot Nr_3 \sim 8N_g$, where $N_g=$ number of
G-vectors for the charge density), and $T_{us}=$ CPU time required by
ultrasoft contribution (if any).

The time $T_{scf}$ for calculation of potential from charge density is
$$
T_{scf} = d_2 \cdot Nr_1 \cdot Nr_2 \cdot Nr_3 + d_3 \cdot
          Nr_1 \cdot Nr_2 \cdot Nr_3 \cdot
          \log(Nr_1 \cdot Nr_2 \cdot Nr_3) 
$$
where $d_1$, $d_2$ are prefactors.

\subsection{Memory requirement}

A typical \texttt{pw.x} run will require a maximum memory in the order
of $O$ double precision complex numbers, where
$$
O = m \cdot M \cdot N + P \cdot N + p \cdot N_1 \cdot N_2 \cdot N_3
    + q \cdot Nr_1 \cdot Nr_2 \cdot Nr_3
$$
with $m$, $p$, $q=$ small factors; all other variables have the same
meaning as above.
Note that if the $\Gamma$-point only ($\mathbf{q}=0$) is used to
sample the Brillouin Zone, the value of $N$ will be cut into half.

Code \texttt{memory.x} yields a rough estimate of the required memory 
and checks for the validity of the input data file as well.
Use it exactly as \texttt{pw.x}.

The memory required by the phonon code follows the same patterns,
with somewhat larger factors $m$, $p$, $q$.

\subsection{File space requirement}

A typical \texttt{pw.x} run will require an amount of temporary disk
space in the order of $O$ double precision complex numbers:
$$
O = N_k \cdot M \cdot N + q \cdot Nr_1 \cdot Nr_2 \cdot Nr_3
$$
where $q=2 \cdot \mathtt{mixing\_ndim}$ (number of iterations used in
self-consistency, default value $=8$) if \texttt{disk\_io} is set to
\texttt{'high'} or not specified;
$q=0$ if \texttt{disk\_io='low'} or \texttt{'minimal'}.

\subsection{Parallelization issues}

\texttt{pw.x} can run in principle on any number of processors (up to
\texttt{maxproc}, presently fixed at 128 in \texttt{PW/para.f90}). 
The $N_p$ processors can be divided into $N_{pk}$ pools of $N_{pr}$
processors, $N_p=N_{pk}*N_{pr}$. 
The k-points are divided across $N_{pk}$ pools (``k-point
parallelization''), while both R- and G-space grids are divided across
the $N_{pr}$ processors of each pool (``PW parallelization'').
There is also a third level of parallelization, on the number of
bands, but it is currently confined to the calculation of a few
quantities that would not be parallelized at all otherwise.

The effectiveness of parallelization depends on the size and type of
the system and on a judicious choice of the $N_{pk}$ and $N_{pr}$:

\begin{itemize}
  \item
    k-point parallelization is very effective if $N_{pk}$ is a divisor
    of the number of k-points (linear speedup guaranteed), \emph{but}
    it does not reduce the amount of memory per processor taken by the
    calculation.
    As a consequence, large systems may not fit into memory.
  \item
    PW parallelization works well if $N_{pr}$ is a divisor of both
    dimensions along the $z$ axis of the FFT grids, $N_3$ and $Nr_3$
    (which may coincide).
    It does not scale so well as k-point parallelization, but it
    reduces both CPU time AND memory (the latter almost linearly).
  \item
    Optimal serial performances are achieved when the data are as much
    as possible kept into the cache.
    As a side effect, one can achieve better than linear scaling with
    the number of processors, thanks to the increase in serial speed
    coming from the reduction of data size (making it easier for the
    machine to keep data in the cache).  
\end{itemize}

Note that for each system there is an optimal range of number of 
processors on which to run the job.
A too large number of processors will yield performance degradation,
or may cause the parallelization algorithm to fail in distributing
properly R- and G-space grids.

Note also that Beowulf-style machines (PC clusters) may have
disappointing parallelization performances unless they have a decent
communication hardware (at least Gigabit ethernet).
Do not expect good scaling with cheap hardware: plane-wave
calculations are not at all an "embarrassing parallel" problem. 
Note that multiprocessor motherboards for Intel Pentium CPUs typically
have just one memory bus for all processors.
This dramatically slows down any code doing  massive access to memory
(such as PWscf and other plane-wave codes) that runs on processors of
the same motherboard.

\clearpage

\section{Troubleshooting}

Almost all problems in PWscf arise from incorrect input data and
result in error stops. Error messages should be self-explanatory, 
but unfortunately this is not always true. If the code issues a
warning messages and continues, pay attention to it but do not
assume that something is necessarily wrong in your calculation:
most warning messages signal harmless problems.

Note for PC Linux clusters in parallel execution: in at least some 
versions of MPICH, the current directory is set to the directory where
the \emph{executable code} resides, instead of being set to the
directory where the code is executed.
This MPICH weirdness may cause unexpected failures in some
postprocessing codes (i.e., \texttt{chdens.x}) that expect a data file
in the current directory.
Workaround: use symbolic links, or copy the executable to the current
directory.

Typical \texttt{pw.x} and/or \texttt{ph.x} (mis-)behavior:

\paragraph{\texttt{pw.x} yields a message like ``error while loading
           shared libraries: \dots{} cannot open shared object file''
           and does not start.}

Possible reasons: 

\begin{itemize}
  \item
    If you are running on the same machines on which the code was
    compiled, this is a library configuration problem.
    The solution is machine-dependent.
    On Linux, find the path to the missing libraries; then either add
    it to file \texttt{/etc/ld.so.conf} and run \texttt{ldconfig}
    (must be done as root), or add it to variable
    \texttt{LD\_LIBRARY\_PATH} and export it.
    Another possibility is to load non-shared version of libraries
    (ending with \texttt{.a}) instead of shared ones (ending with
    \texttt{.so}).
  \item
    If you are \emph{not} running on the same machines on which the
    code was compiled: you need either to have the same shared
    libraries installed on both machines, or to load statically all
    libraries (using appropriate compiler or loader options).
\end{itemize}

\paragraph{\texttt{pw.x} stops with error in reading.}

There is an error in the input data.
Usually it is a misspelled namelist variable, or an empty input file.
Note that out-of-bound indices in dimensioned variables read in the
namelist may cause the code to crash with really mysterious error
messages.
Also note that input data files containing \texttt{\^{}M} (Control-M)
characters at the end of lines (typically, files coming from Windows
PC) may yield error in reading.
If none of the above applies and the code stops at the first namelist
and you are running on a PC cluster: your communication library
(LAM-MPI or MPICH) might not be properly configured to allow input
redirection (so that you are effectively reading an empty file).
Inquire with your local computer wizard.

\paragraph{\texttt{pw.x} mumbles something like ``cannot recover'' or
           ``error reading recover file''.}

You have a bad restart file from a preceding failed execution.
Remove all files \texttt{restart*} in \texttt{outdir}.

\paragraph{\texttt{pw.x} stops with error in cdiagh or cdiaghg.}

Possible reasons:
\begin{itemize}
  \item
    serious error in data, such as bad atomic positions or bad crystal
    structure/supercell;
  \item
    a bad PP (for instance, with a ghost);
  \item
    a failure of the algorithm performing subspace diagonalization. 
    The LAPACK algorithms used by cdiagh or cdiaghg are very robust 
    and extensively tested. Still, it may seldom happen that such
    algorithms fail, for unclar reasons. The error is not reproducible
    on a different architecture and usually disappears if the
    calculation is repeated with even minimal changes in its
    parameters. If this happen, use conjugate-gradient diagonalization
   (\texttt{diagonalization='cg'}) and see what happens. 
  \item
    HP-Compaq alphas with \texttt{cxml} libraries: try to use compiled
    BLAS and LAPACK (or better, ATLAS) instead of those contained in
    \texttt{cxml} (just load them before \texttt{cxml}).
\end{itemize}

\paragraph{\texttt{pw.x} crashes with ``floating invalid''.}

If this happens on HP-Compaq\\ True64 Alpha machines with an old
version of the compiler: the compiler is most likely buggy.
Otherwise, move to next item.

\paragraph{\texttt{pw.x} crashes with no error message at all.}

This happens quite often in parallel execution, or under a batch
queue, or if you are writing the output to a file.
When the program crashes, part of the output, including the error
message, may be lost, or hidden into error files where nobody looks
into.
It is the fault of the operating system, not of the code.
Try to run interactively and to write to the screen.
If this doesn't help, move to next point.

\paragraph{\texttt{pw.x} crashes with ``segmentation fault'' or
           similarly obscure messages.}

Possible reasons:
\begin{itemize}
  \item
    nonexistent or non accessible {\tt outdir}. 
    Note that in parallel execution, {\tt outdir} must exist and be
    accessible to all active processors.
  \item
    too much RAM memory requested (see next item).
  \item
    if you are using highly optimized mathematical libraries, verify
    that they are designed for your hardware.
    In particular, for Intel compiler and MKL libraries, verify that
    you loaded the correct set of CPU-specific MKL libraries.
  \item
    buggy compiler.
    If you are using Portland or Intel compilers on Linux PC's or
    clusters, see the ``Installation issues'' section.
\end{itemize}

\paragraph{\texttt{pw.x} works for simple systems, but not for large
           systems or whenever more RAM is needed.}

Possible solutions:
\begin{itemize}
  \item
    increase the amount of RAM you are authorized to use (which may be
    much smaller than the available RAM).
    Inquire with your system administrator if you don't know what to
    do.
  \item
    reduce \texttt{nbnd} to the strict minimum, or reduce the cutoffs,
    or the cell size.
  \item
    use conjugate-gradient or DIIS diagonalization\\
    (\texttt{diagonalization='cg'} or \texttt{'diis'}): slower, but
    requires less memory.
  \item
    in parallel execution, use more processors, or use the same number
    of processors with less pools.
    Remember that parallelization with respect to k-points (pools)
    does not distribute memory: parallelization with respect to
    \textbf{R}- (and \textbf{G}-) space does.
  \item
    IBM only (32-bit machines): if you need more than 256 MB you must
    specify it at link time (option \texttt{-bmaxdata}).
  \item
    buggy compiler.
    Some versions of Portland compiler on Linux PC's or clusters have
    this problem.
\end{itemize}

\paragraph{\texttt{pw.x} runs but nothing happens.}

Possible reasons:
\begin{itemize}
  \item
    in parallel execution, the code died on just one processor.
    Unpredictable behavior may follow.
  \item
    in serial execution, the code encountered a floating-point error
    and goes on producing NaN's (Not a Number) forever unless
    exception handling is on (and usually it isn't).
    In both cases, look for one of the reasons given above.
\end{itemize}

\paragraph{\texttt{pw.x} yields weird results.}

Possible solutions:
\begin{itemize}
  \item
    if this happen after a change in the code or in compilation or
    preprocessing options, try \texttt{make clean} and recompile.
    The \texttt{make} command should take care of all dependencies,
    but do not rely too heavily on it. 
    If the problem persists, \texttt{make clean} and recompile with
    reduced optimization level.
  \item
    maybe your input data are weird.
\end{itemize}

\paragraph{\texttt{pw.x} stops with error message ``the system is
           metallic, specify occupations''.}

You did not specify state occupations, but you need to, since your
system appears to have an odd number of electrons.
The variable controlling occupations is, obviously,
\texttt{occupations} in namelist \texttt{\&SYSTEM}.
The default, \texttt{occupations='fixed'}, occupies the lowest
\texttt{nelec/2} states and works only for insulators with a gap.
In all other cases, use \texttt{'smearing'} or \texttt{'tetrahedra'}.
If you choose \texttt{'smearing'}, you must provide the smearing
width: \texttt{degauss} (required!) and the smearing type:
\texttt{smearing} (optional).
If you choose \texttt{'tetrahedra'}, you need to specify a suitable
uniform k-point grid (card \texttt{K\_POINTS} with option
\texttt{automatic}).
See file \texttt{INPUT\_PW} for more details.

\paragraph{\texttt{pw.x} stops with ``unexpected error'' in
           \texttt{efermi}.}

Possible reasons:
\begin{itemize}
  \item
    serious error in data, such as bad number of electrons,
    insufficient number of bands, absurd value of broadening, or too
    few tetrahedra;
  \item
    the Fermi energy is found by bisection assuming that the
    integrated DOS $N(E)$ is an increasing function of the energy. 
    This is {\em not} guaranteed for Methfessel-Paxton smearing of
    order 1 and can give problems when very few k-points are used.
    Use some other smearing function: simple Gaussian broadening or,
    better, Marzari-Vanderbilt ``cold smearing''.
\end{itemize}

\paragraph{the FFT grids in \texttt{pw.x} are machine-dependent.}

Yes, they are!
The code automatically chooses the smallest grid that is compatible
with the specified cutoff in the specified cell, \emph{and} is an
allowed value for the FFT library used.
Most FFT libraries are implemented, or perform well, only with
dimensions that factors into products of small numers (2, 3, 5
typically, sometimes 7 and 11).
Different FFT libraries follow different rules and thus different
dimensions can result for the same system on different machines (or
even on the same machine, with a different FFT).
See function \texttt{allowed} in \texttt{Modules/fft\_scalar.f90}.

As a consequence, the energy may be slightly different on different
machines.
The only piece that depends explicitely on the grid parameters is the
XC part of the energy that is computed numerically on the grid.
The differences should be small, though, expecially for LDA
calculations.

Manually setting the FFT grids to a desired value is possible, but
slightly tricky, using input variables \texttt{nr1, nr2, nr3} and
\texttt{nr1s, nr2s, nr3s}.
The code will still increase them if not acceptable.
Automatic FFT grid dimensions are slightly overestimated, so one may
try --- very carefully --- to reduce them a little bit.
The code will stop if too small values are required, it will waste CPU
time and memory for too large values.

Note that in parallel execution, it is very convenient to have FFT
grid dimensions along z that are a multiple of the number of
processors.

\paragraph{``warning: symmetry operation \# N not allowed''.}

This is not an error.
\texttt{pw.x} determines first the symmetry operations (rotations)
of the Bravais lattice; then checks which of these are symmetry
operations of the system (including if needed fractional
translations).
This is done by rotating (and translating if needed) the atoms in
the unit cell and verifying if the rotated unit cell coincides
with the original one. 

If a symmetry operations contain a
fractional translation that is incompatible with the FFT grid,
it is discarded in order to prevent problems with symmetrization.
Typical fractional translations are 1/2 or 1/3 of a lattice
vector. If the FFT grid dimension along that direction is not 
divisible respectively by 2 or by 3, the symmetry operation will 
not transform the FFT grid into itself.

\paragraph{\texttt{pw.x} doesn't find all the symmetries you
           expected.}

See above to learn how PWscf finds symmetry operations.
Some of them might be missing because:
\begin{itemize}
  \item
    the number of significant figures in the atomic positions is not
    large enough.
    In file \texttt{PW/eqvect.f90}, the variable \texttt{accep} is
    used to decide whether a rotation is a symmetry operation.
    Its current value ($10^{-5}$) is quite strict: a rotated atom must
    coincide with another atom to 5 significant digits.
    You may change the value of \texttt{accep} and recompile.
  \item
    they are not acceptable symmetry operations of the Bravais
    lattice.
    This is the case for C$_{60}$, for instance: the $I_h$ icosahedral
    group of C$_{60}$ contains 5-fold rotations that are incompatible
    with translation symmetry.
  \item
    the system is rotated with respect to symmetry axis.
    For instance: a C$_{60}$ molecule in the fcc lattice will have 24
    symmetry operations ($T_h$ group) only if the double bond is
    aligned along one of the crystal axis; if C$_{60}$ is rotated in
    some arbitrary way, \texttt{pw.x} may not find any symmetry, apart
    from inversion.
  \item
    they contain a fractional translation that is incompatible with
    the FFT grid (see previous paragraph).
    Note that if you change cutoff or unit cell volume, the
    automatically computed FFT grid changes, and this may explain
    changes in symmetry (and in the number of k-points as a
    consequence) for no apparent good reason (only if you have
    fractional translations in the system, though).
  \item
    a fractional translation, without rotation, is a symmetry
    operation of the system. This means that the cell is actually 
    a supercell. In this case, all symmetry operations containing 
    fractional translations are disabled.
    The reason is that in this rather exotic case there is no simple
    way to select those symmetry operations forming a true group, in
    the mathematical sense of the term.
\end{itemize}

\paragraph{the CPU time is time-dependent!}

Yes it is!
On most machines and on most operating systems, depending on machine
load, on communication load (for parallel machines), on various other
factors (including maybe the phase of the moon), reported CPU times
may vary quite a lot for the same job.
Also note that what is printed is supposed to be the CPU time per
process, but on at least some machines it is actually the wall time.

\paragraph{on parallel execution, \texttt{pw.x} stops complaining that
           ``some processors have no planes'' or ``smooth planes'' or
           some other strange error.}

Your system does not require that many processors: reduce the number
of processors to a more sensible value.
In particular, both $N_3$ and $Nr_3$ must be $\geq N_{pr}$ (see the
``Performance Issues'' section, and in particular ``Parallelization
issues'', for the meaning of these variables).

\paragraph{``warning : N eigenvectors not converged ...''}

This is a warning message that can be safely ignored if it
is not present in the last steps of self-consistency. If it 
is still present in the last steps of self-consistency, and
if the number of unconverged eigevector is a significant 
part of the total,it may signal serious trouble in self-consistency
(see next point) or something badly wrong in input data.

\paragraph{self-consistency is slow or does not converge.}

Reduce \texttt{mixing\_beta} from the default value (0.7) to $\sim
0.3-0.1$ or smaller, or try a different \texttt{mixing\_mode}.
You may also try to increase \texttt{mixing\_ndim} to more than 8
(default value).
Beware: the larger \texttt{mixing\_ndim}, the larger the amount of
memory you need.

If the above doesn't help: verify if your system is metallic or is
close to a metallic state, especially if you have few k-points.
If the highest occupied and lowest unoccupied state(s) keep exchanging
place during self-consistency, convergence is hopeless: a typical sign
is that the self-consistency error goes down, down, down, than all of
a sudden up again, and so on.
Usually this behaviour disappears if you add a few empty bands and a
broadening.

Specific to US PP: the presence of negative charge density regions due
to either the pseudization procedure of the augmentation part or to
truncation at finite cutoff may give convergence problems.
Raising the \texttt{ecutrho} cutoff for charge density will usually
help, especially in gradient-corrected calculations.

\paragraph{structural optimization goes wild after first or second
           step.}

The algorithm used in structural optimization is not very robust.
If you start too far away from minimum, it may lead to badly wrong
atomic positions.
Restart from a better starting point.

\paragraph{structural optimization stops with ``unexpected error''.}

The line minimization algorithm (\texttt{linmin}) looks for a minimum
--- using atomic positions and forces at the current and preceding
step, and 3rd- or 2nd-degree polynomial interpolation --- along a
given direction (determined by the BFGS algorithm).
If this minimum turns out to be where it shouldn't be (for instance,
on the wrong side with respect to forces), the code stops with
``unexpected error''.

In practice, this means that there is an inconsistency between forces
and energies (see next item).
If the error arises when forces are very small, just increase the
thresholds for convergence of structural optimization.

\paragraph{structural optimization is slow or does not converge.}

Close to convergence the self-consistency error in forces may become
large with respect to the value of forces.
The resulting mismatch between forces and energies may confuse the
line minimization algorithm, which assumes consistency between the
two.
The code reduces the starting self-consistency threshold
\texttt{conv\_thr} when approaching the minimum energy configuration,
up to a factor defined by \texttt{upscale}.
Reducing \texttt{conv\_thr} (or increasing \texttt{upscale}) yields a
smoother structural optimization, but if \texttt{conv\_thr} becomes
too small, electronic self-consistency may not converge.
You may also increase variables \texttt{etot\_conv\_thr} and
\texttt{forc\_conv\_thr} that determine the threshold for convergence
(the default values are quite strict).

A limitation to the accuracy of forces comes from the absence of
perfect translational invariance.
If we had only the Hartree potential, our PW calculation would be
translationally invariant to machine precision.
The presence of an exchange-correlation potential introduces Fourier
components in the potential that are not in our basis set.
This loss of precision (more serious for gradient-corrected
functionals) translates into a slight but detectable loss of
translational invariance (the energy changes if all atoms are
displaced by the same quantity, not commensurate with the FFT grid).
This puts a limit to the accuracy of forces.
The situation improves somewhat by increasing the \texttt{ecutrho}
cutoff.

Also note that in many systems you may have ``floppy'' low-energy
modes, that make very difficult --- and of little use anyway --- to
reach a well converged structure, no matter what.

\paragraph{\texttt{ph.x} stops with ``error reading file''.}

The data file produced by \texttt{pw.x} is bad or incomplete or
produced by an incompatible version of the code.
In parallel execution: the number of processors and pools for the
phonon run should be the same as for the self-consistent run; all
files must be visible to all processors.

\paragraph{\texttt{ph.x} mumbles something like ``cannot recover'' or
           ``error reading recover file''.}

You have a bad restart file from a preceding failed execution.
Remove all files \texttt{recover*} in \texttt{outdir}.

\paragraph{\texttt{ph.x} does not yield acoustic modes with $\omega=0$
           at $\mathbf{q}=0$.}

This may not be an error: the Acoustic Sum Rule (ASR) is never exactly
verified, because the system is never exactly translationally
invariant as it should be (see the discussion above).
The calculated frequency of the acoustic mode is typically less than
10 cm$^{-1}$, but in some cases it may be much higher, up to 100
cm$^{-1}$.
The ultimate test is to diagonalize the dynamical matrix with program
\texttt{dynmat.x}, imposing the ASR.
If you obtain an acoustic mode with a much smaller $\omega$ (let's say
$<1 \textrm{cm}^{-1}$) with all other modes virtually unchanged, you
can trust your results.

\paragraph{\texttt{ph.x} yields really lousy phonons, with bad or
           negative frequencies or wrong symmetries or gross ASR
           violations.}

Possible reasons:
\begin{itemize}
  \item
    wrong data file file read.
  \item
    wrong atomic masses given in input will yield wrong frequencies
    (but the content of file {\tt fildyn} should be valid, since the
    force constants, not the dynamical matrix, are written to file).
  \item
    convergence threshold for either SCF ({\tt conv\_thr}) or phonon
    calculation ({\tt tr2\_ph}) too large (try to reduce them).
  \item
    maybe your system \emph{does} have negative or strange phonon
    frequencies, with the approximations you used.
    A negative frequency signals a mechanical instability of the
    chosen structure.
    Check that the structure is reasonable, and check the following
    parameters:
    \begin{itemize}
      \item The cutoff for wavefunctions, \texttt{ecutwfc}
      \item For US PP: the cutoff for the charge density,
            \texttt{ecutrho}
      \item The k-point grid, especially for metallic systems!
    \end{itemize}
\end{itemize}

\paragraph{``Wrong degeneracy'' error in star\_q.}

Verify the \textbf{q}-point for which you are calculating phonons.
In order to check whether a symmetry operation belongs to the small
group of \textbf{q}, the code compares \textbf{q} and the rotated
\textbf{q}, with an acceptance tolerance of $10^{-5}$ (set in routine
\texttt{PW/eqvect.f90}).
You may enconter trouble if your \textbf{q}-point differs from a
high-symmetry point by an amount in that order of magnitude.

\end{document}
