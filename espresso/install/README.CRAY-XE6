Info by Filippo Spiga, Jun. 2013, valid for any version of QE after 5.


Machine name    : MonteRosa at CSCS(CH)
Machine spec    : http://user.cscs.ch/hardware/monte_rosa_cray_xe6/index.html


1. Compile the code

Suggested compiler : 

## PGI ##

 

## INTEL ##



## CRAY ##



## GNU ##


Executables will be located under "./bin"


2. Good practices

- if your calculation is FFT-bounded Use the hybrid version of code. The 
reason is that there are 1 GByte RAM/core and if you put 32 MPI in a 
single node you are going to stress the GEMINI interconnection.

- CRAY LIBSCI library works well for all the compilers, I do not see any
advantages to use ACML explicitly.

- use ScaLAPACK (--with-scalapack), let the configure detect and use the 
default library (it will be the CRAY libsci, the make.sys will not show
anything because everything is done by the CRAY wrapper ftn/cc).

- try ELPA library (--with-elpa) but check properly results

- The environment is exported automatically by 'sbatch' during the 
submission operation. So check to have loaded properly the right modules.



3. Example scripts 

This script run pw.x over 6400 cores (800 MPI, 8 MPI per node, 4
OMP per MPI thread). The flag ""

#SBATCH --job-name="QE-BENCH-SPIGA"
#SBATCH --nodes=200
#SBATCH --ntasks-per-node=8
#SBATCH --cpus-per-task=4
#SBATCH --time=06:00:00
#SBATCH --output=QE-BENCH.%j.o
#SBATCH --error=QE-BENCH.%j.e
#SBATCH --account=<...>

export OMP_NUM_THREADS=4
aprun -n $SLURM_NPROCS -N 8 -d 4 -S 2 ./pw.x -input SiGe25.in -npool 4 | tee out


This script run pw.x over 6400 cores (800 MPI, 4 MPI per node, 8
OMP per MPI thread).

#!/bin/bash
#SBATCH --job-name="QE-BENCH-SPIGA"
#SBATCH --nodes=200
#SBATCH --ntasks-per-node=4
#SBATCH --cpus-per-task=8
#SBATCH --time=06:00:00
#SBATCH --output=QE-BENCH.%j.o
#SBATCH --error=QE-BENCH.%j.e
#SBATCH --account=<...>

export OMP_NUM_THREADS=8
aprun -n $SLURM_NPROCS -N 4 -d 8 -S 1./pw.x -input SiGe25.in -npool 4 | tee out


The flag "-S" is the number of MPI tasks per NUMA node. Each XE6 nodes 
contains 2 x 16-core CPU, 4 NUMA nodes  in total. The value of "-S" has 
to change according to the combination MPIxOMP in the node:

-N 8 -d 4 --> -S 2 (because there are 8 MPI to distribute across 4 NUMA nodes)
-N 4 -d 8 --> -S 1 (because there are 4 MPI to distribute across 4 NUMA nodes)


NOTE (1): "-S" is optional. The resource manager should be enough smart to
      place the MPi processes in the right place but I never double-check
      
NOTE (2): other two useful options for aprun are:

-ss	(Optional) Demands strict memory containment per NUMA node. 

-cc	(Optional) Controls how tasks are bound to cores and NUMA nodes. 
               The recommend setting for most codes is -cc cpu which restricts 
               each task to run on a specific core. 
               
Try and use them wisely.

          

4. Benchmarks 
          