Info by Filippo Spiga, Jun. 2013, valid for any version of QE after 5.


0. Tested environments

Machine name    : TODI at CSCS (CH)
Machine spec    : http://user.cscs.ch/hardware/todi_cray_xk7/index.html

Machine name    : TITAN at Oak Ridge National laboratory (USA)
Machine spec    : https://www.olcf.ornl.gov/computing-resources/titan-cray-xk7/

IMPORTANT NOTE: other CRAY XK7 systems might have different modules, please
                check for equivalent if the ones mentioned are missing

 

1. Compile the code

Suggested compiler : PGI


## PGI (usually the default after login) ##

module load cudatoolkit
module unload atp totalview-support xt-totalview hss-llm
module load gcc/4.4.4

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --disable-magma --enable-phigemm  --with-scalapack --disable-profiling  ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-scalapack.x

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --disable-magma --enable-phigemm  --with-scalapack --disable-profiling --with-elpa  ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-elpa.x

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --enable-magma --enable-phigemm  --without-scalapack --disable-profiling  --with-internal-cblas ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-magma.x


## INTEL ##

module switch PrgEnv-pgi PrgEnv-intel
module load cudatoolkit
module unload atp totalview-support xt-totalview hss-llm
module load gcc/4.4.4

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --disable-magma --enable-phigemm  --with-scalapack --disable-profiling  ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-scalapack.x

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --disable-magma --enable-phigemm  --with-scalapack --disable-profiling --with-elpa  ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-elpa.x

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --enable-magma --enable-phigemm  --without-scalapack --disable-profiling  --with-internal-cblas ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-magma.x


## CRAY ##

module load cudatoolkit
module unload atp totalview-support xt-totalview hss-llm
module load gcc/4.4.4

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --disable-magma --enable-phigemm  --with-scalapack --disable-profiling  --disable-wrappers ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-scalapack.x

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --disable-magma --enable-phigemm  --with-scalapack --disable-profiling --with-elpa  --disable-wrappers ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-elpa.x

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --enable-magma --enable-phigemm  --without-scalapack --disable-profiling --disable-wrappers   --with-internal-cblas ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-magma.x


## GNU ##

module switch PrgEnv-pgi PrgEnv-gnu
module unload gcc
module load gcc/4.4.4
module load cudatoolkit
module unload atp totalview-support xt-totalview hss-llm
module load fftw

export CC="cc"
export FC="ftn"
export CXX="CC"
export MPICC="cc"
export MPIF90="ftn"
export CFLAGS="-O3 -fopenmp"
export FCLAGS="-O3 -fopenmp"
export CXXFLAGS="-O3 -fopenmp"

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --disable-magma --enable-phigemm  --with-scalapack --disable-profiling  ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-scalapack.x

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --disable-magma --enable-phigemm  --with-scalapack --disable-profiling --with-elpa  ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-elpa.x

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --enable-magma --enable-phigemm  --without-scalapack --disable-profiling  --with-internal-cblas ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-magma.x



IMPORTANT NOTE : Executables will be located under "./bin"

IMPORTANT NOTE : only pw-gpu.x, neb-gpu.x, ph-gpu.x use extensively 
                 the GPU card in multiple sections of the code. All 
                 the other executable exploit the GPU only by the 
                 phiGEMM library (for now)


    
2. Good practices

- Each NVIDIA Tesla K20 GPU has 6 GB of memory on the card. Better to limit 
  the number of MPI per node (so the number of MPI sharing the same GPU) 
  to 2.

- If the calculation is not too memory demanding, it is possible to increase 
  the ratio MPI:GPU up to 4. The new Hyper-Q technology will help to leverage 
  and exploit the GPU at its best.
  
- In order to share the GPU between multiple MPI processes within the node is 
  mandatory to export the variable CRAY_CUDA_PROXY ("export CRAY_CUDA_PROXY=1")



3. Example scripts 

#SBATCH --job-name="QE-BENCH-SPIGA"
#SBATCH --nodes=64
# REMEMBER: --ntasks-per-node * --cpus-per-task <= 16
#SBATCH --ntasks-per-node=2
#SBATCH --cpus-per-task=8
#SBATCH --time=02:00:00
#SBATCH --output=QE-BENCH.%j.o
#SBATCH --error=QE-BENCH.%j.e
#SBATCH --account=<...>

echo "The current job ID is $SLURM_JOB_ID"
echo "Running on $SLURM_NNODES nodes"
echo "Using $SLURM_NTASKS_PER_NODE tasks per node"
echo "A total of $SLURM_NPROCS tasks is used"

export OMP_NUM_THREADS=8
export CRAY_CUDA_PROXY=1

export MALLOC_MMAP_MAX_=0
export MALLOC_TRIM_THRESHOLD_=536870912
#export MPICH_VERSION_DISPLAY=1
#export MPICH_ENV_DISPLAY=1

aprun -n $SLURM_NPROCS -N 2 -d 8 ./pw.x -input <...> | tee out



4. Benchmarks 
